# MiniOneRec åŸºç¡€æ¨¡å‹è¯´æ˜

## ğŸ“‹ æ¦‚è¿°

MiniOneRec **ä¸é™åˆ¶ç‰¹å®šçš„åŸºç¡€æ¨¡å‹**ï¼Œæ”¯æŒä»»ä½• HuggingFace çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal Language Modelï¼‰ã€‚ä½†æ ¹æ®ä»£ç å®ç°å’Œæ¨èå®è·µï¼ŒæŸäº›æ¨¡å‹æœ‰æ›´å¥½çš„å…¼å®¹æ€§å’Œæ€§èƒ½ã€‚

## âœ… æ”¯æŒçš„åŸºç¡€æ¨¡å‹

### 1. **Qwen ç³»åˆ—ï¼ˆæ¨èï¼‰**

**æ¨èç†ç”±**ï¼š
- æ–‡æœ¬åµŒå…¥é˜¶æ®µé»˜è®¤ä½¿ç”¨ Qwen æ¨¡å‹
- ä»£ç ä¸­æœ‰ä¸“é—¨é’ˆå¯¹ Qwen çš„ä¼˜åŒ–
- åœ¨ä¸­æ–‡åœºæ™¯ä¸‹è¡¨ç°ä¼˜ç§€

**å¯ç”¨æ¨¡å‹**ï¼š
- `Qwen/Qwen2-0.5B-Instruct`ï¼ˆä»£ç ç¤ºä¾‹ä¸­ä½¿ç”¨ï¼‰
- `Qwen/Qwen2-1.5B-Instruct`
- `Qwen/Qwen2-3B-Instruct` âœ… **æ¨èï¼ˆèµ„æºå—é™åœºæ™¯ï¼‰**
- `Qwen/Qwen2.5-3B-Instruct` âœ… **å¼ºçƒˆæ¨èï¼ˆæ–°ç‰ˆæœ¬ï¼Œæ€§èƒ½æ›´å¥½ï¼‰**
- `Qwen/Qwen2-7B-Instruct` âœ… **A100 æ¨èï¼ˆæ€§èƒ½ä¸èµ„æºå¹³è¡¡ï¼‰**
- `Qwen/Qwen2.5-7B-Instruct` âœ… **A100 å¼ºçƒˆæ¨èï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰**
- `Qwen/Qwen2-14B-Instruct`ï¼ˆå¤šå¡ A100 æ¨èï¼‰
- `Qwen/Qwen2.5-14B-Instruct`ï¼ˆå¤šå¡ A100 æ¨èï¼‰
- å…¶ä»– Qwen ç³»åˆ—æ¨¡å‹

**æ¨¡å‹é€‰æ‹©å»ºè®®**ï¼š

| GPU ç±»å‹ | æ¨èæ¨¡å‹ | æ˜¾å­˜å ç”¨ | æ€§èƒ½ |
|---------|---------|---------|------|
| **å•å¡ A100 (80GB)** | `Qwen2.5-7B-Instruct` âœ… | ~12GB (LoRA) | â­â­â­â­â­ |
| **å•å¡ A100 (80GB)** | `Qwen2.5-3B-Instruct` | ~8GB (LoRA) | â­â­â­â­ |
| **å¤šå¡ A100** | `Qwen2.5-14B-Instruct` | ~24GB (LoRA) | â­â­â­â­â­ |
| **èµ„æºå—é™** | `Qwen2.5-3B-Instruct` | ~8GB (LoRA) | â­â­â­â­ |

**æ³¨æ„**ï¼š
- **Qwen2.5 ç³»åˆ—**æ˜¯ Qwen2 çš„å‡çº§ç‰ˆï¼Œæ€§èƒ½æ›´å¥½ï¼Œ**å¼ºçƒˆæ¨èä½¿ç”¨**
- å¯¹äº **A100 GPU**ï¼Œæ¨èä½¿ç”¨ **7B æ¨¡å‹**ï¼ˆæ€§èƒ½æ›´å¥½ï¼Œæ˜¾å­˜å……è¶³ï¼‰
- å¦‚æœæ˜¾å­˜å—é™æˆ–éœ€è¦æ›´å¿«è®­ç»ƒï¼Œ**3B æ¨¡å‹**ä¹Ÿæ˜¯å¾ˆå¥½çš„é€‰æ‹©

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
python sft.py \
    --base_model Qwen/Qwen2-1.5B-Instruct \
    ...
```

### 2. **LLaMA ç³»åˆ—**

**æ”¯æŒæƒ…å†µ**ï¼š
- ä»£ç ä¸­æœ‰ä¸“é—¨é’ˆå¯¹ LLaMA çš„å¤„ç†é€»è¾‘
- åœ¨ tokenization æ—¶ä¼šè‡ªåŠ¨å»æ‰ç¬¬ä¸€ä¸ª tokenï¼ˆBOS tokenï¼‰

**å¯ç”¨æ¨¡å‹**ï¼š
- `decapoda-research/llama-7b-hf`ï¼ˆä»£ç ç¤ºä¾‹ä¸­æåˆ°ï¼‰
- `meta-llama/Llama-2-7b-hf`
- `meta-llama/Llama-2-13b-hf`
- `meta-llama/Llama-3-8B`
- å…¶ä»– LLaMA ç³»åˆ—æ¨¡å‹

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
python sft.py \
    --base_model decapoda-research/llama-7b-hf \
    ...
```

**ç‰¹æ®Šå¤„ç†**ï¼š
```python
# ä»£ç ä¸­è‡ªåŠ¨å¤„ç† LLaMA çš„ç‰¹æ®Š tokenization
if base_model.lower().find("llama") > -1:
    prefixID = [tokenizer(_).input_ids[1:] for _ in info]  # å»æ‰ç¬¬ä¸€ä¸ª token
else:
    prefixID = [tokenizer(_).input_ids for _ in info]
```

### 3. **GPT2 ç³»åˆ—**

**æ”¯æŒæƒ…å†µ**ï¼š
- ä»£ç ä¸­æœ‰é’ˆå¯¹ GPT2 çš„ç‰¹æ®Šå¤„ç†
- prefix_index è®¾ç½®ä¸º 4ï¼ˆå…¶ä»–æ¨¡å‹ä¸º 3ï¼‰

**å¯ç”¨æ¨¡å‹**ï¼š
- `gpt2`
- `gpt2-medium`
- `gpt2-large`
- `gpt2-xl`

**ç‰¹æ®Šå¤„ç†**ï¼š
```python
if base_model.lower().find("gpt2") > -1:
    prefix_index = 4  # GPT2 çš„ç‰¹æ®Šå‰ç¼€ç´¢å¼•
else:
    prefix_index = 3
```

### 4. **å…¶ä»–æ¨¡å‹**

ç†è®ºä¸Šæ”¯æŒä»»ä½• HuggingFace çš„ `AutoModelForCausalLM` æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
- **Mistral** ç³»åˆ—
- **Baichuan** ç³»åˆ—
- **ChatGLM** ç³»åˆ—
- **InternLM** ç³»åˆ—
- å…¶ä»–å¼€æºå› æœè¯­è¨€æ¨¡å‹

## ğŸ” ä»£ç ä¸­çš„æ¨¡å‹æ£€æµ‹é€»è¾‘

### 1. LLaMA æ£€æµ‹

```python
# åœ¨å¤šä¸ªæ–‡ä»¶ä¸­éƒ½æœ‰æ­¤æ£€æµ‹
if base_model.lower().find("llama") > -1:
    # LLaMA ç‰¹æ®Šå¤„ç†
    prefixID = [tokenizer(_).input_ids[1:] for _ in info]
```

**ä½ç½®**ï¼š
- `minionerec_trainer.py` (line 543)
- `evaluate.py` (line 75)
- `LogitProcessor.py` (é—´æ¥ä½¿ç”¨)

### 2. GPT2 æ£€æµ‹

```python
if base_model.lower().find("gpt2") > -1:
    prefix_index = 4
else:
    prefix_index = 3
```

**ä½ç½®**ï¼š
- `minionerec_trainer.py` (line 548)
- `evaluate.py` (line 81)
- `LogitProcessor.py` (line 35)

### 3. é€šç”¨å¤„ç†

å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†çš„ Transformers å¤„ç†æ–¹å¼ï¼š
```python
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.bfloat16,
)
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
```

## ğŸ“Š æ¨¡å‹é€‰æ‹©å»ºè®®

### æ ¹æ®èµ„æºé€‰æ‹©

| æ¨¡å‹å¤§å° | æ¨èæ¨¡å‹ | æ˜¾å­˜éœ€æ±‚ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|
| **å°æ¨¡å‹** (< 2B) | Qwen2-0.5B, Qwen2-1.5B | 4-8GB | å¿«é€Ÿå®éªŒã€èµ„æºå—é™ |
| **ä¸­ç­‰æ¨¡å‹** (2-4B) | **Qwen2-3B** âœ… | 10-16GB | **å¹³è¡¡æ€§èƒ½å’Œèµ„æºï¼Œæ¨è** |
| **ä¸­ç­‰æ¨¡å‹** (4-7B) | Qwen2-7B, LLaMA-7B | 16-24GB | å¹³è¡¡æ€§èƒ½å’Œèµ„æº |
| **å¤§æ¨¡å‹** (> 7B) | Qwen2-14B, LLaMA-13B | 32GB+ | è¿½æ±‚æœ€ä½³æ€§èƒ½ |

### æ ¹æ®åœºæ™¯é€‰æ‹©

1. **ä¸­æ–‡æ¨èåœºæ™¯**ï¼šæ¨èä½¿ç”¨ **Qwen ç³»åˆ—**
   - ä¸­æ–‡ç†è§£èƒ½åŠ›å¼º
   - æ–‡æœ¬åµŒå…¥é˜¶æ®µä¹Ÿä½¿ç”¨ Qwen

2. **è‹±æ–‡æ¨èåœºæ™¯**ï¼šå¯ä»¥ä½¿ç”¨ **LLaMA ç³»åˆ—**
   - è‹±æ–‡è¡¨ç°ä¼˜ç§€
   - ç¤¾åŒºæ”¯æŒå¥½

3. **å¿«é€ŸéªŒè¯**ï¼šä½¿ç”¨ **å°æ¨¡å‹**ï¼ˆå¦‚ Qwen2-0.5Bï¼‰
   - è®­ç»ƒé€Ÿåº¦å¿«
   - èµ„æºæ¶ˆè€—å°‘

## ğŸ”§ æ¨¡å‹ä½¿ç”¨æµç¨‹

### é˜¶æ®µ 1ï¼šæ–‡æœ¬åµŒå…¥ï¼ˆæ¨èä½¿ç”¨ Qwenï¼‰

**ä½œç”¨**ï¼šå°†å•†å“æ–‡æœ¬ï¼ˆæ ‡é¢˜+æè¿°ï¼‰ç¼–ç ä¸ºå‘é‡è¡¨ç¤ºï¼Œç”¨äºåç»­çš„ SID æ„å»ºã€‚

**æ¨èæ¨¡å‹**ï¼š
- **Qwen ç³»åˆ—**ï¼ˆé»˜è®¤æ¨èï¼‰
  - `Qwen/Qwen2-7B-Instruct` âœ… **æ¨è**ï¼ˆå¹³è¡¡è´¨é‡å’Œé€Ÿåº¦ï¼‰
  - `Qwen/Qwen2-14B-Instruct`ï¼ˆæœ€ä½³è´¨é‡ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢ï¼‰
  - `Qwen/Qwen2-3B-Instruct`ï¼ˆé€Ÿåº¦å¿«ï¼Œè´¨é‡ç•¥ä½ï¼‰

**æ‰§è¡Œå‘½ä»¤**ï¼š
```bash
# æ–‡æœ¬åµŒå…¥é˜¶æ®µå»ºè®®ä½¿ç”¨ Qwen
bash rq/text2emb/amazon_text2emb.sh \
     --plm_name qwen \
     --plm_checkpoint Qwen/Qwen2-7B-Instruct \
     --dataset Office_Products \
     --root ./data/Amazon/index
```

**æŠ€æœ¯ç»†èŠ‚**ï¼š
- ä½¿ç”¨æ¨¡å‹çš„ `last_hidden_state` è¿›è¡Œå¹³å‡æ± åŒ–ï¼ˆmasked mean poolingï¼‰
- è¾“å‡ºç»´åº¦ï¼šé€šå¸¸ä¸º 1024 æˆ– 2048 ç»´ï¼ˆå–å†³äºæ¨¡å‹ï¼‰
- æ”¯æŒå¤š GPU å¹¶è¡Œå¤„ç†ï¼ˆä½¿ç”¨ Accelerateï¼‰
- è¾“å‡ºæ–‡ä»¶ï¼š`{dataset}.emb-{plm_name}-td.npy`

**æ¨¡å‹é€‰æ‹©å»ºè®®**ï¼š
- **æ–‡æœ¬åµŒå…¥é˜¶æ®µ**å¯ä»¥ä½¿ç”¨**æ›´å¤§çš„æ¨¡å‹**ï¼ˆå¦‚ 7B+ï¼‰ï¼Œå› ä¸ºåªéœ€è¦æ¨ç†ï¼Œä¸éœ€è¦è®­ç»ƒ
- **SFT/RL é˜¶æ®µ**å¯ä»¥ä½¿ç”¨**è¾ƒå°çš„æ¨¡å‹**ï¼ˆå¦‚ 3Bï¼‰ï¼Œå› ä¸ºéœ€è¦è®­ç»ƒï¼Œèµ„æºæ¶ˆè€—æ›´å¤§
- æ–‡æœ¬åµŒå…¥é˜¶æ®µä½¿ç”¨çš„æ¨¡å‹**ä¸éœ€è¦**ä¸ SFT é˜¶æ®µçš„åŸºç¡€æ¨¡å‹ç›¸åŒï¼Œä½†ä½¿ç”¨ç›¸åŒæ¨¡å‹å¯èƒ½æœ‰ä¸€è‡´æ€§ä¼˜åŠ¿

**æ”¯æŒçš„åµŒå…¥æ¨¡å‹**ï¼š
- `qwen`ï¼šQwen ç³»åˆ—æ¨¡å‹ï¼ˆæ¨èï¼‰
- ç†è®ºä¸Šæ”¯æŒä»»ä½• HuggingFace çš„ç¼–ç å™¨æ¨¡å‹ï¼Œä½†éœ€è¦ä¿®æ”¹ä»£ç 

### é˜¶æ®µ 2ï¼šSFTï¼ˆå¯é€‰æ‹©ä»»æ„æ¨¡å‹ï¼‰

**ä½œç”¨**ï¼šç›‘ç£å¾®è°ƒï¼Œè®­ç»ƒæ¨¡å‹ç”Ÿæˆå•†å“ SIDã€‚

**æ¨¡å‹é€‰æ‹©**ï¼š
- å¯ä»¥ä½¿ç”¨ä¸æ–‡æœ¬åµŒå…¥é˜¶æ®µä¸åŒçš„æ¨¡å‹
- æ¨èä½¿ç”¨ä¸­ç­‰å¤§å°çš„æ¨¡å‹ï¼ˆ3B-7Bï¼‰ä»¥å¹³è¡¡æ€§èƒ½å’Œèµ„æº
- å°æ¨¡å‹ï¼ˆ< 2Bï¼‰é€‚åˆå¿«é€ŸéªŒè¯
- å¤§æ¨¡å‹ï¼ˆ> 7Bï¼‰é€‚åˆè¿½æ±‚æœ€ä½³æ€§èƒ½

**æ‰§è¡Œå‘½ä»¤**ï¼š
```bash
# SFT é˜¶æ®µå¯ä»¥ä½¿ç”¨ä¸åŒçš„åŸºç¡€æ¨¡å‹
python sft.py \
    --base_model Qwen/Qwen2-7B-Instruct \  # æˆ– LLaMAã€GPT2 ç­‰
    --train_file train.csv \
    --eval_file valid.csv \
    --output_dir ./output/sft \
    --sid_index_path ./data/Amazon/index/Office_Products.index.json \
    --item_meta_path ./data/Amazon/index/Office_Products.item.json \
    ...
```

**å…³é”®å‚æ•°**ï¼š
- `--base_model`ï¼šåŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆHuggingFace æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„ï¼‰
- `--freeze_LLM`ï¼šæ˜¯å¦å†»ç»“ LLM å‚æ•°ï¼Œåªè®­ç»ƒæ–°æ·»åŠ çš„ SID token embeddings
- `--sid_index_path`ï¼šSID ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆ`.index.json`ï¼‰
- `--item_meta_path`ï¼šå•†å“å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆ`.item.json`ï¼‰

### é˜¶æ®µ 3ï¼šRLï¼ˆä½¿ç”¨ SFT è¾“å‡ºçš„æ¨¡å‹ï¼‰

**ä½œç”¨**ï¼šå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨èæ€§èƒ½ã€‚

**æ¨¡å‹è¦æ±‚**ï¼š
- **å¿…é¡»**ä½¿ç”¨ SFT é˜¶æ®µè®­ç»ƒå¥½çš„æ¨¡å‹
- ä¸èƒ½ç›´æ¥ä½¿ç”¨åŸå§‹åŸºç¡€æ¨¡å‹ï¼ˆéœ€è¦å…ˆç»è¿‡ SFTï¼‰

**æ‰§è¡Œå‘½ä»¤**ï¼š
```bash
# RL é˜¶æ®µä½¿ç”¨ SFT è®­ç»ƒå¥½çš„æ¨¡å‹
python rl.py \
    --model_path ./output/sft/final_checkpoint \  # SFT è¾“å‡º
    --train_file train.csv \
    --eval_file valid.csv \
    --output_dir ./output/rl \
    --info_file ./data/Amazon/info/Office_Products_5_2016-10-2018-11.txt \
    --sid_index_path ./data/Amazon/index/Office_Products.index.json \
    --item_meta_path ./data/Amazon/index/Office_Products.item.json \
    ...
```

**å…³é”®å‚æ•°**ï¼š
- `--model_path`ï¼šSFT é˜¶æ®µè¾“å‡ºçš„æ¨¡å‹è·¯å¾„ï¼ˆå¿…é¡»ï¼‰
- `--reward_type`ï¼šå¥–åŠ±å‡½æ•°ç±»å‹ï¼ˆ`rule` æˆ– `sasrec`ï¼‰
- `--num_generations`ï¼šæ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„å€™é€‰æ•°é‡ï¼ˆé»˜è®¤ 16ï¼‰

## âš™ï¸ æ¨¡å‹å…¼å®¹æ€§å¤„ç†

### 1. Tokenizer å¤„ç†

```python
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token  # è®¾ç½® pad token
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"  # å·¦å¡«å……ï¼ˆç”¨äºç”Ÿæˆä»»åŠ¡ï¼‰
```

### 2. æ–° Token æ·»åŠ 

```python
# æ·»åŠ  SID tokens åˆ°è¯è¡¨
if sid_index_path and os.path.exists(sid_index_path):
    new_tokens = token_extender.get_new_tokens()
    if new_tokens:
        tokenizer.add_tokens(new_tokens)
        model.resize_token_embeddings(len(tokenizer))
```

### 3. æ¨¡å‹åŠ è½½

```python
# æ”¯æŒ bfloat16 ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.bfloat16,  # èŠ‚çœæ˜¾å­˜
)
```

## ğŸ“ å®é™…ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šä½¿ç”¨ Qwen2-3Bï¼ˆæ¨èï¼‰

```bash
# 1. æ–‡æœ¬åµŒå…¥ï¼ˆä½¿ç”¨ Qwenï¼Œå¯ä»¥ç”¨æ›´å¤§çš„æ¨¡å‹ï¼‰
bash rq/text2emb/amazon_text2emb.sh \
     --plm_name qwen \
     --plm_checkpoint Qwen/Qwen2-7B-Instruct  # åµŒå…¥é˜¶æ®µå¯ç”¨å¤§æ¨¡å‹

# 2. SFTï¼ˆä½¿ç”¨ Qwen2-3Bï¼‰
python sft.py \
    --base_model Qwen/Qwen2-3B-Instruct \
    --train_file train.csv \
    --output_dir ./output/sft \
    --batch_size 512 \
    --micro_batch_size 8

# 3. RLï¼ˆä½¿ç”¨ SFT è¾“å‡ºï¼‰
python rl.py \
    --model_path ./output/sft/final_checkpoint \
    ...
```

### ç¤ºä¾‹ 1.5ï¼šä½¿ç”¨ Qwen2-7B

```bash
# 1. æ–‡æœ¬åµŒå…¥ï¼ˆä½¿ç”¨ Qwenï¼‰
bash rq/text2emb/amazon_text2emb.sh \
     --plm_name qwen \
     --plm_checkpoint Qwen/Qwen2-7B-Instruct

# 2. SFTï¼ˆä½¿ç”¨ Qwenï¼‰
python sft.py \
    --base_model Qwen/Qwen2-7B-Instruct \
    --train_file train.csv \
    --output_dir ./output/sft

# 3. RLï¼ˆä½¿ç”¨ SFT è¾“å‡ºï¼‰
python rl.py \
    --model_path ./output/sft/final_checkpoint \
    ...
```

### ç¤ºä¾‹ 2ï¼šä½¿ç”¨ LLaMA-7B

```bash
# 1. æ–‡æœ¬åµŒå…¥ï¼ˆä»å¯ä½¿ç”¨ Qwenï¼‰
bash rq/text2emb/amazon_text2emb.sh \
     --plm_name qwen \
     --plm_checkpoint Qwen/Qwen2-7B-Instruct

# 2. SFTï¼ˆä½¿ç”¨ LLaMAï¼‰
python sft.py \
    --base_model decapoda-research/llama-7b-hf \
    --train_file train.csv \
    --output_dir ./output/sft

# 3. RL
python rl.py \
    --model_path ./output/sft/final_checkpoint \
    ...
```

### ç¤ºä¾‹ 3ï¼šä½¿ç”¨å°æ¨¡å‹å¿«é€ŸéªŒè¯

```bash
# ä½¿ç”¨ Qwen2-0.5B å¿«é€ŸéªŒè¯
python sft.py \
    --base_model Qwen/Qwen2-0.5B-Instruct \
    --train_file train.csv \
    --output_dir ./output/sft \
    --freeze_LLM False
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æ¨¡å‹å¤§å°ä¸èµ„æº

- **å°æ¨¡å‹**ï¼ˆ< 2Bï¼‰ï¼šé€‚åˆå¿«é€Ÿå®éªŒï¼Œä½†æ€§èƒ½å¯èƒ½æœ‰é™
- **ä¸­ç­‰æ¨¡å‹**ï¼ˆ2-7Bï¼‰ï¼šå¹³è¡¡æ€§èƒ½å’Œèµ„æºï¼Œæ¨èä½¿ç”¨
- **å¤§æ¨¡å‹**ï¼ˆ> 7Bï¼‰ï¼šæ€§èƒ½æœ€å¥½ï¼Œä½†éœ€è¦å¤§é‡èµ„æº

### 2. æ¨¡å‹æ ¼å¼è¦æ±‚

- å¿…é¡»æ˜¯ **HuggingFace æ ¼å¼**çš„æ¨¡å‹
- æ”¯æŒ `AutoModelForCausalLM` æ¥å£
- éœ€è¦æä¾›å¯¹åº”çš„ tokenizer

### 3. ç‰¹æ®Š Token å¤„ç†

- ä¸åŒæ¨¡å‹çš„ tokenization å¯èƒ½ä¸åŒ
- ä»£ç å·²è‡ªåŠ¨å¤„ç† LLaMA å’Œ GPT2 çš„ç‰¹æ®Šæƒ…å†µ
- å…¶ä»–æ¨¡å‹ä½¿ç”¨æ ‡å‡†å¤„ç†æ–¹å¼

### 4. æ˜¾å­˜ä¼˜åŒ–

- ä½¿ç”¨ `torch_dtype=torch.bfloat16` èŠ‚çœæ˜¾å­˜
- æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰
- æ”¯æŒ DeepSpeed ZeROï¼ˆé€šè¿‡é…ç½®æ–‡ä»¶ï¼‰

## ğŸ”— æ¨èçš„æ¨¡å‹ä¸‹è½½

### Qwen ç³»åˆ—
- [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)
- [Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)
- [Qwen2-3B-Instruct](https://huggingface.co/Qwen/Qwen2-3B-Instruct) âœ… **æ¨è**
- [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)

### LLaMA ç³»åˆ—
- [LLaMA-7B](https://huggingface.co/decapoda-research/llama-7b-hf)
- [Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)

### GPT2 ç³»åˆ—
- [gpt2](https://huggingface.co/gpt2)

## ğŸ“ æ€»ç»“

**MiniOneRec æ”¯æŒçš„åŸºç¡€æ¨¡å‹**ï¼š

1. âœ… **Qwen ç³»åˆ—**ï¼ˆæ¨èï¼Œç‰¹åˆ«æ˜¯ä¸­æ–‡åœºæ™¯ï¼‰
2. âœ… **LLaMA ç³»åˆ—**ï¼ˆè‹±æ–‡åœºæ™¯è¡¨ç°å¥½ï¼‰
3. âœ… **GPT2 ç³»åˆ—**ï¼ˆè½»é‡çº§é€‰æ‹©ï¼‰
4. âœ… **å…¶ä»– HuggingFace å› æœè¯­è¨€æ¨¡å‹**ï¼ˆç†è®ºä¸Šéƒ½æ”¯æŒï¼‰

**é€‰æ‹©å»ºè®®**ï¼š
- **ä¸­æ–‡æ¨è**ï¼šä¼˜å…ˆä½¿ç”¨ Qwen ç³»åˆ—
- **è‹±æ–‡æ¨è**ï¼šå¯ä»¥ä½¿ç”¨ LLaMA ç³»åˆ—
- **å¿«é€ŸéªŒè¯**ï¼šä½¿ç”¨å°æ¨¡å‹ï¼ˆå¦‚ Qwen2-0.5Bï¼‰
- **å¹³è¡¡é€‰æ‹©**ï¼š**æ¨èä½¿ç”¨ Qwen2-3B** âœ…ï¼ˆæ€§èƒ½ä¸èµ„æºçš„æœ€ä½³å¹³è¡¡ï¼‰
- **æœ€ä½³æ€§èƒ½**ï¼šä½¿ç”¨å¤§æ¨¡å‹ï¼ˆå¦‚ Qwen2-7B æˆ–æ›´å¤§ï¼‰

**å…³é”®ç‚¹**ï¼š
- æ–‡æœ¬åµŒå…¥é˜¶æ®µå»ºè®®ä½¿ç”¨ Qwenï¼ˆä»£ç é»˜è®¤ï¼‰
- SFT å’Œ RL é˜¶æ®µå¯ä»¥ä½¿ç”¨ä»»æ„å…¼å®¹çš„æ¨¡å‹
- ä»£ç å·²è‡ªåŠ¨å¤„ç†ä¸åŒæ¨¡å‹çš„ç‰¹æ®Š tokenization éœ€æ±‚

---

## ğŸ¯ æ¨¡å‹é€‰æ‹©æŒ‡å—ï¼ˆA100 GPUï¼‰

### å¿«é€Ÿæ¨è

**å•å¡ A100 (80GB) æ¨è**ï¼š

| æ¨¡å‹ | æ¨èåº¦ | æ˜¾å­˜å ç”¨ (LoRA) | è®­ç»ƒé€Ÿåº¦ | æ€§èƒ½ | æ¨èåœºæ™¯ |
|------|--------|----------------|---------|------|---------|
| **Qwen2.5-7B-Instruct** | â­â­â­â­â­ | ~12GB | å¿« | ä¼˜ç§€ | **æœ€ä½³é€‰æ‹©**ï¼ˆæ€§èƒ½ä¸èµ„æºå¹³è¡¡ï¼‰ |
| **Qwen2.5-3B-Instruct** | â­â­â­â­ | ~8GB | å¾ˆå¿« | è‰¯å¥½ | å¿«é€Ÿå®éªŒã€æ˜¾å­˜å—é™ |
| Qwen2-7B-Instruct | â­â­â­â­ | ~12GB | å¿« | ä¼˜ç§€ | ç¨³å®šå¯é  |
| Qwen2-3B-Instruct | â­â­â­ | ~8GB | å¾ˆå¿« | è‰¯å¥½ | èµ„æºå—é™ |

### Qwen2.5-3B-Instruct vs Qwen2.5-7B-Instruct

| ç‰¹æ€§ | Qwen2.5-3B | Qwen2.5-7B |
|------|-----------|-----------|
| **å‚æ•°é‡** | 3B | 7B |
| **æ˜¾å­˜å ç”¨ (LoRA)** | ~8GB | ~12GB |
| **è®­ç»ƒé€Ÿåº¦** | æ›´å¿« | å¿« |
| **æ¨ç†é€Ÿåº¦** | æ›´å¿« | å¿« |
| **æ€§èƒ½** | è‰¯å¥½ | ä¼˜ç§€ |
| **æ¨èåœºæ™¯** | å¿«é€Ÿå®éªŒã€æ˜¾å­˜å—é™ | ç”Ÿäº§ç¯å¢ƒã€è¿½æ±‚æ€§èƒ½ |

### Qwen2.5 ç³»åˆ— vs Qwen2 ç³»åˆ—

**Qwen2.5 ç³»åˆ—ä¼˜åŠ¿**ï¼š
- âœ… **æ€§èƒ½æ›´å¥½**ï¼šåœ¨ SuperCLUE ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´ä¼˜
- âœ… **ä¸­æ–‡èƒ½åŠ›æ›´å¼º**ï¼šåœ¨ä¸­æ–‡æ–‡æœ¬ç†è§£ä¸åˆ›ä½œæ–¹é¢å¾—åˆ†æ›´é«˜
- âœ… **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ›´å¥½çš„å¤šè¯­è¨€ç†è§£èƒ½åŠ›
- âœ… **é•¿æ–‡æœ¬å¤„ç†**ï¼šæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡

**å»ºè®®**ï¼š**ä¼˜å…ˆé€‰æ‹© Qwen2.5 ç³»åˆ—**ï¼ˆå¦‚ `Qwen2.5-7B-Instruct` æˆ– `Qwen2.5-3B-Instruct`ï¼‰

### é€‰æ‹©å»ºè®®

**åœºæ™¯ 1ï¼šè¿½æ±‚æœ€ä½³æ€§èƒ½ï¼ˆA100 å•å¡ï¼‰**

**æ¨è**ï¼š`Qwen/Qwen2.5-7B-Instruct`

**ç†ç”±**ï¼š
- A100 æ˜¾å­˜å……è¶³ï¼ˆ80GBï¼‰ï¼Œ7B æ¨¡å‹å®Œå…¨æ²¡é—®é¢˜
- æ€§èƒ½æ˜æ˜¾ä¼˜äº 3B æ¨¡å‹
- LoRA æ˜¾å­˜å ç”¨ä»… ~12GBï¼Œè¿˜æœ‰å¤§é‡ä½™é‡

**åœºæ™¯ 2ï¼šå¿«é€Ÿå®éªŒæˆ–æ˜¾å­˜å—é™**

**æ¨è**ï¼š`Qwen/Qwen2.5-3B-Instruct`

**ç†ç”±**ï¼š
- è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼ˆçº¦å¿« 30-50%ï¼‰
- æ˜¾å­˜å ç”¨æ›´å°‘ï¼ˆ~8GB vs ~12GBï¼‰
- æ€§èƒ½ä»ç„¶å¾ˆå¥½ï¼ˆåœ¨ 3B çº§åˆ«ä¸­æ’åç¬¬ä¸€ï¼‰

### æ€§èƒ½å‚è€ƒ

**SuperCLUE ç«¯ä¾§ 5B çº§åˆ«æ¦œå•ï¼ˆ2025å¹´3æœˆï¼‰**ï¼š

| æ’å | æ¨¡å‹ | æ€»åˆ† | ä¸­æ–‡æ–‡æœ¬ç†è§£ä¸åˆ›ä½œ |
|------|------|------|------------------|
| 1 | **Qwen2.5-3B-Instruct** | 22.18 | 73.49 |
| ... | ... | ... | ... |

**ç»“è®º**ï¼šQwen2.5-3B-Instruct åœ¨ 3B çº§åˆ«æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚

### æœ€ç»ˆå»ºè®®

å¯¹äº **A100 GPU**ï¼š

1. **é¦–é€‰**ï¼š`Qwen2.5-7B-Instruct` â­â­â­â­â­
   - æ€§èƒ½æœ€å¥½
   - æ˜¾å­˜å……è¶³
   - è®­ç»ƒé€Ÿåº¦å¯æ¥å—

2. **æ¬¡é€‰**ï¼š`Qwen2.5-3B-Instruct` â­â­â­â­
   - å¦‚æœé€‰æ‹© 3Bï¼Œè¿™æ˜¯æœ€å¥½çš„é€‰æ‹©
   - è®­ç»ƒæ›´å¿«
   - æ€§èƒ½ä»ç„¶å¾ˆå¥½

3. **å…³é”®ç‚¹**ï¼š
   - âœ… **ä¼˜å…ˆé€‰æ‹© Qwen2.5 ç³»åˆ—**ï¼ˆæ–°ç‰ˆæœ¬ï¼Œæ€§èƒ½æ›´å¥½ï¼‰
   - âœ… **A100 æ¨è 7B**ï¼ˆæ€§èƒ½ä¸èµ„æºçš„æœ€ä½³å¹³è¡¡ï¼‰
   - âœ… **3B ä¹Ÿæ˜¯å¾ˆå¥½çš„é€‰æ‹©**ï¼ˆå¦‚æœè¿½æ±‚æ›´å¿«è®­ç»ƒï¼‰

