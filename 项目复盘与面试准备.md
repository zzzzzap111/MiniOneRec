# 🚀 基于LLM语义ID的生成式电商推荐系统优化：从复现到改进的完整复盘

## 📝 1. 项目背景与动机 (Why this project?)

传统的推荐系统（如DeepFM, DIN）主要依赖ID Embedding，存在两个核心痛点：
1.  **冷启动问题**：新商品没有交互历史，无法学习有效的ID Embedding。
2.  **语义缺失**：ID只是一个数字索引，无法利用商品标题、描述中的丰富语义信息。

随着LLM的发展，直接用LLM做推荐（LLM4Rec）成为趋势，但直接输入长文本会导致推理延迟过高。因此，本项目基于**MiniOneRec**框架，探索**生成式推荐（Generative Recommendation）**范式，核心思想是：**将商品映射为短小精悍的语义ID（Semantic ID），让LLM直接生成用户感兴趣的下一个商品ID。**

---

## 🏗️ 2. 第一阶段：基线模型构建（Version 1）

### 2.1 语义ID构建 (The Foundation)
**目标**：把亚马逊数据的长文本商品压缩成短ID。
*   **方法**：使用 **RQ-VAE (Residual Quantized Variational Autoencoder)**。
*   **流程**：
    1.  利用 **Qwen2.5-3B** 提取商品标题和描述的文本Embedding。
    2.  训练RQ-VAE，设置码本大小 $K=256$，层数 $D=3$。
    3.  **结果**：每个商品被表示为 `(c1, c2, c3)` 的3个token序列。相比直接使用文本，序列长度压缩了98%以上。

### 2.2 SFT 监督微调 (Baseline SFT)
**目标**：让LLM学会“用户历史 -> 下一个商品SID”的预测任务。
*   **配置**：
    *   **基座模型**：Qwen2.5-3B-Instruct。
    *   **微调技术**：**LoRA** (Rank=64, Alpha=128, Dropout=0.05)。
    *   **硬件**：A100-40GB 单卡。
    *   **训练目标**：标准的 Next Token Prediction (NTP)。
*   **结果**：HR@10 = **0.106**，NDCG@10 = **0.083**。
*   **观察**：模型能够初步理解推荐任务，但倾向于推荐热门商品（Popularity Bias）。

### 2.3 RL 强化学习 (Baseline RL)
**目标**：通过强化学习直接优化推荐指标。
*   **算法**：**GRPO (Group Relative Policy Optimization)**。
    *   *选择GRPO理由*：相比PPO，GRPO不需要额外的Value Network（Critic），节省显存，适合大模型训练。
*   **策略**：针对每个Query生成 $G=16$ 个候选结果，组内计算优势（Advantage）。
*   **奖励函数**：简单的规则奖励（命中+1，未命中0）。
*   **结果**：HR@10 提升至 **0.121** (+14.2%)。

---

## 🔍 3. 第二阶段：问题分析与优化策略（Version 2）

在分析基线（Version 1）的Bad Case后，我发现了两个主要问题：
1.  **SFT阶段的头部效应**：训练Loss下降很快，但模型过度拟合高频商品，对长尾商品（Long-tail）预测极差。这是因为交叉熵损失函数对简单样本（热门商品）和困难样本（冷门商品）一视同仁。
2.  **RL奖励过于粗糙**：基线的奖励函数只看“对不对”，忽略了“排在哪”和“新旧程度”。业务上，排在第1位和第10位的价值完全不同，且平台需要扶持新品。

基于此，我进行了针对性的改进（Version 2）：

### 3.1 SFT 改进：引入 Focal Loss
为了解决头部效应，我在SFT阶段重构了损失函数，引入 **Focal Loss**：
$$ FL(p_t) = -\alpha (1 - p_t)^\gamma \log(p_t) $$
*   **参数设置**：$\gamma = 2.0$。
*   **原理**：对于模型预测概率 $p_t$ 较高的样本（通常是热门商品，容易预测），权重 $(1-p_t)^\gamma$ 会急剧减小；对于预测概率低的困难样本（长尾商品），权重相对保留。
*   **效果**：模型被迫关注那些“难啃”的长尾样本。
*   **SFT结果提升**：HR@10 从 0.106 -> **0.112**。

### 3.2 RL 改进：业务导向的奖励函数设计
我重新设计了GRPO的Reward Function，使其更符合真实的电商业务逻辑：

1.  **基础奖励**：$R_{base} = 1.0$ （如果命中 Ground Truth）。
2.  **排序衰减惩罚 (Ranking Decay)**：
    *   *基线*：线性或者无区分。
    *   *改进*：$R_{rank} = -0.15 \times \log(rank)$。
    *   *逻辑*：用户注意力呈对数衰减。Top 1 和 Top 2 的差距巨大，但 Top 9 和 Top 10 的差距很小。
3.  **多样性激励 (Diversity Bonus)**：
    *   *改进*：$R_{div} = 0.1 \times \text{Unique\_Categories\_Count}$。
    *   *逻辑*：直接统计推荐列表中包含的不同商品类目数量。每增加一个类目奖励0.1分。这个系数设计得很克制（即使Top-10全是不同类目，总奖励也才1.0），确保模型不会为了凑多样性而牺牲准确性，但在同等准确率下会优先选择覆盖面更广的结果。

### 3.3 最终结果与对比
经过上述优化，最终模型（Version 2）表现如下：
*   **HR@10**: **0.135** (相比基线 +20.5%)
*   **NDCG@10**: **0.108** (相比基线 +21.3%)

---

## ⚔️ 4. 面试高频问题预演 (Q&A)

在面试中，面试官会针对你的改进点进行深挖。以下是你必须准备好的答案：

### Q1: 为什么要用语义ID（SID）？直接用Item ID不行吗？
**答**：
直接用Item ID有两个硬伤：
1.  **Token量级爆炸**：电商平台通常有千万级商品，直接作为词表（Vocabulary）会导致Softmax层显存溢出。
2.  **语义割裂**：ID `1001` 和 `1002` 在数字上接近，但在语义上可能一个是“口红”，一个是“轮胎”。
**SID的优势**是：通过RQ-VAE量化，我们将千万级商品映射到了 $256 \times 3$ 的有限组合空间中，既利用了文本的语义相似性（相似商品的SID编码也相似），又控制了词表大小。

### Q2: 你在SFT阶段引入Focal Loss，为什么 $\gamma$ 选2.0？你是怎么调参的？
**答**：
这是一个平衡点。
*   如果 $\gamma=0$，就是标准的Cross Entropy，模型会过分关注头部热门商品。
*   如果 $\gamma$ 太大（比如5.0），模型会过度关注离群点（Outliers）或标注错误的脏数据，导致训练不收敛。
*   我进行了几组实验（0.5, 1.0, 2.0, 5.0），发现 $\gamma=2.0$ 时，模型在保持头部商品准确率的同时，对长尾商品的Recall提升最明显。

### Q3: 你的RL阶段用了GRPO，为什么不用PPO？
**答**：
这是一个**计算效率**的权衡。
*   **PPO** 需要训练一个 Critic 网络（Value Function）来估计基线，这意味着我们需要同时加载 Policy Model 和 Value Model，在A100单卡上显存压力极大。
*   **GRPO (Group Relative Policy Optimization)** 不需要 Critic 网络。它通过对同一个Query采样一组（Group=16）输出，用这组输出的平均奖励作为Baseline。
*   公式上：$Advantage_i = \frac{Reward_i - Mean(Group\_Rewards)}{Std(Group\_Rewards)}$。
*   这不仅节省了近一半的显存，而且在生成式任务中，组内对比（Group Comparison）往往比绝对价值估计更准确。

### Q4: 你的排序惩罚为什么用 $-0.15 \times \log(rank)$ 而不是线性的？
**答**：
这是基于**用户行为心理学**的考量。
在推荐列表中，用户的注意力是**非线性衰减**的。用户极其在意 Top 1 结果，但对于 Top 9 和 Top 10 的区别感知不强。
*   线性惩罚（如 $-0.1 \times rank$）意味着第1名和第2名的差距，与第9名和第10名的差距是一样的，这不符合业务事实。
*   对数惩罚（Log）拉大了头部位置的权重，迫使模型极力争夺 Top 1-3 的位置，这直接带来了 NDCG 指标的大幅提升。

### Q5: 你引入多样性激励，不会把用户不喜欢的类目硬推给他吗？
**答**：
这是一个平衡点。我的多样性激励系数比较小（0.1），而基础正确性奖励是 1.0。
这意味着：模型首先要保证**相关性**（即预测的商品在Ground Truth或极其相似），在此基础上，如果能通过**不同类目**来满足用户需求，才会获得额外加分。
这本质上是在**Recall Set**内部做Re-ranking，而不是胡乱推荐不相关的类目。

### Q6: 奖励函数 (Reward) 怎么设计？
**答**：
我的项目中设计了**多目标融合奖励**（Version 2）：
$$R_{total} = R_{base} + R_{rank} + R_{div}$$
1.  **正确性 ($R_{base}$)**：如果生成的SID对应商品在Ground Truth集合中，+1.0 分，否则 0 分。
2.  **位置感知 ($R_{rank}$)**：虽然是生成式，但我也在乎顺序。如果命中，给予 $-0.15 \times \log(rank)$ 惩罚。排名越靠后，扣分越多。
3.  **多样性 ($R_{div}$)**：$0.1 \times \text{Count}(\text{Unique Categories})$。每增加一个类目奖励0.1分，鼓励覆盖更多子类目。
**设计原则**：准确性为主，多样性为辅，兼顾用户体验。

---

### 💡 总结一句话介绍项目
> "这是一个基于 **Qwen2.5** 大模型和 **语义ID** 技术的生成式推荐系统。针对传统推荐缺乏语义理解和头部效应严重的问题，我通过 **RQ-VAE** 构建语义索引，并在SFT阶段引入 **Focal Loss** 解决长尾分布，在RL阶段利用 **GRPO** 结合 **对数排序惩罚** 与 **多样性激励** 优化奖励函数，最终在A100单卡上实现了相比基线模型 **HR@10 提升20.5%** 的效果。"

