# MiniOneRec å¼ºåŒ–å­¦ä¹ å®ç°è¯¦è§£

## ğŸ“‹ æ¦‚è¿°

MiniOneRec ä½¿ç”¨ **GRPO (Group Relative Policy Optimization)** ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œè¿™æ˜¯åŸºäº DeepSeekMath è®ºæ–‡æå‡ºçš„æ–¹æ³•ï¼Œä¸“é—¨é€‚é…æ¨èç³»ç»Ÿåœºæ™¯ã€‚

## ğŸ¯ æ ¸å¿ƒç®—æ³•ï¼šGRPO

### GRPO ç®—æ³•åŸç†

GRPO æ˜¯ä¸€ç§**ç»„å†…ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–**æ–¹æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

1. **ç»„å†…å½’ä¸€åŒ–**ï¼šä¸ºæ¯ä¸ª prompt ç”Ÿæˆå¤šä¸ªå€™é€‰ï¼ˆé€šå¸¸ 16 ä¸ªï¼‰ï¼Œåœ¨ç»„å†…è¿›è¡Œå¥–åŠ±å½’ä¸€åŒ–
2. **ç›¸å¯¹ä¼˜åŠ¿**ï¼šä½¿ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿è€Œéç»å¯¹å¥–åŠ±ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
3. **KL æƒ©ç½š**ï¼šé€šè¿‡ KL æ•£åº¦æƒ©ç½šä¿æŒç­–ç•¥æ¥è¿‘å‚è€ƒæ¨¡å‹ï¼Œé¿å…è¿‡åº¦åç¦»

### ç®—æ³•æµç¨‹

```
1. ç”Ÿæˆé˜¶æ®µï¼šä¸ºæ¯ä¸ª prompt ç”Ÿæˆ num_generations ä¸ªå€™é€‰æ¨è
2. å¥–åŠ±è®¡ç®—ï¼šè®¡ç®—æ¯ä¸ªå€™é€‰çš„å¥–åŠ±
3. ç»„å†…å½’ä¸€åŒ–ï¼šåœ¨ç»„å†…è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå½’ä¸€åŒ–å¥–åŠ±
4. ä¼˜åŠ¿è®¡ç®—ï¼šadvantages = (rewards - mean) / (std + eps)
5. æŸå¤±è®¡ç®—ï¼šç»“åˆä¼˜åŠ¿å‡½æ•°å’Œ KL æ•£åº¦æƒ©ç½š
6. åå‘ä¼ æ’­ï¼šæ›´æ–°æ¨¡å‹å‚æ•°
```

## ğŸ”§ å®ç°æ¶æ„

### 1. æ ¸å¿ƒç»„ä»¶

#### ReReTrainer ç±»
```python
class ReReTrainer(Trainer):
    """
    åŸºäº GRPO çš„æ¨èç³»ç»Ÿè®­ç»ƒå™¨
    ç»§æ‰¿è‡ª Transformers Trainer
    """
```

**å…³é”®ç‰¹æ€§**ï¼š
- æ”¯æŒçº¦æŸè§£ç ï¼ˆç¡®ä¿ç”Ÿæˆæœ‰æ•ˆ SIDï¼‰
- æ”¯æŒæŸæœç´¢å’Œé‡‡æ ·ç”Ÿæˆ
- æ”¯æŒå¤šç§å¥–åŠ±å‡½æ•°
- æ”¯æŒåŠ¨æ€é‡‡æ ·ç­–ç•¥

### 2. è®­ç»ƒæµç¨‹

#### æ­¥éª¤ 1ï¼šæ•°æ®å‡†å¤‡

```python
# åŠ è½½å¤šç§æ•°æ®é›†
train_data1 = SidDataset(train_file, ...)           # SID åºåˆ—æ•°æ®
train_data2 = RLTitle2SidDataset(...)                # æ ‡é¢˜åˆ° SID æ˜ å°„
train_data3 = RLSeqTitle2SidDataset(...)             # åºåˆ—æ ‡é¢˜åˆ° SID

train_data = ConcatDataset(train_datasets)
```

**æ•°æ®é›†ç±»å‹**ï¼š
- `SidDataset`ï¼šç”¨æˆ·å†å² SID åºåˆ— â†’ ç›®æ ‡ SID
- `RLTitle2SidDataset`ï¼šå•†å“æ ‡é¢˜ â†’ SID æ˜ å°„ä»»åŠ¡
- `RLSeqTitle2SidDataset`ï¼šåºåˆ—æ ‡é¢˜ â†’ SID æ˜ å°„ä»»åŠ¡

#### æ­¥éª¤ 2ï¼šæ¨¡å‹åˆå§‹åŒ–

```python
trainer = ReReTrainer(
    model=model_path,                    # SFT è®­ç»ƒå¥½çš„æ¨¡å‹
    base_model=model_path,               # åŸºç¡€æ¨¡å‹ï¼ˆç”¨äºå‚è€ƒï¼‰
    reward_funcs=reward_fun,             # å¥–åŠ±å‡½æ•°
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=training_args,
    # ç‰¹æ®Šå‚æ•°
    info_file=info_file,                 # SID ç´¢å¼•æ–‡ä»¶
    prompt2history=prompt2history,       # prompt â†’ history æ˜ å°„
    history2target=history2target,       # history â†’ target æ˜ å°„
    beam_search=True,                    # æ˜¯å¦ä½¿ç”¨æŸæœç´¢
    num_generations=16,                  # æ¯ä¸ª prompt ç”Ÿæˆæ•°é‡
)
```

#### æ­¥éª¤ 3ï¼šç”Ÿæˆå€™é€‰æ¨è

åœ¨ `_prepare_inputs` æ–¹æ³•ä¸­ï¼š

```python
# 1. ä¸ºæ¯ä¸ª prompt ç”Ÿæˆå¤šä¸ªå€™é€‰
if beam_search:
    # ä½¿ç”¨æŸæœç´¢ç”Ÿæˆ
    prompt_completion_ids = unwrapped_model.generate(
        dedup_prompt_ids,
        generation_config=self.generation_config,  # num_beams=16
        logits_processor=self.logits_processor,    # çº¦æŸè§£ç 
    )
else:
    # ä½¿ç”¨é‡‡æ ·ç”Ÿæˆ
    prompt_completion_ids = unwrapped_model.generate(
        prompt_ids,
        generation_config=self.generation_config,
        logits_processor=self.logits_processor,
    )
```

**çº¦æŸè§£ç **ï¼š
- ä½¿ç”¨ `ConstrainedLogitsProcessor` ç¡®ä¿ç”Ÿæˆçš„ token ç¬¦åˆ SID æ ¼å¼
- é€šè¿‡å‰ç¼€å“ˆå¸Œè¡¨å¿«é€ŸæŸ¥æ‰¾æœ‰æ•ˆ token

#### æ­¥éª¤ 4ï¼šè®¡ç®—å¥–åŠ±

```python
# è®¡ç®—æ¯ä¸ªå€™é€‰çš„å¥–åŠ±
rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device)

for i, (reward_func, reward_processing_class) in enumerate(
    zip(self.reward_funcs, self.reward_processing_classes)
):
    if isinstance(reward_func, nn.Module):
        # æ¨¡å‹å¥–åŠ±å‡½æ•°
        rewards_per_func[:, i] = reward_func(**reward_inputs).logits[:, 0]
    else:
        # è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
        output_reward_func = reward_func(prompts=prompts, completions=completions)
        rewards_per_func[:, i] = torch.tensor(output_reward_func, ...)

# åŠ æƒæ±‚å’Œ
rewards = (rewards_per_func * self.reward_weights).sum(dim=1)
```

#### æ­¥éª¤ 5ï¼šç»„å†…å½’ä¸€åŒ–

```python
# å°†å¥–åŠ±æŒ‰ç»„é‡å¡‘ (batch_size * num_generations) â†’ (batch_size, num_generations)
mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)
std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)

# æ‰©å±•å¹¶å½’ä¸€åŒ–
mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)

# è®¡ç®—ä¼˜åŠ¿
advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)
```

**ä¼˜åŠ¿**ï¼š
- ç»„å†…å½’ä¸€åŒ–ä½¿è®­ç»ƒæ›´ç¨³å®š
- ç›¸å¯¹ä¼˜åŠ¿æ¯”ç»å¯¹å¥–åŠ±æ›´æœ‰æ•ˆ
- è‡ªåŠ¨å¤„ç†ä¸åŒ prompt çš„å¥–åŠ±å°ºåº¦å·®å¼‚

#### æ­¥éª¤ 6ï¼šè®¡ç®—æŸå¤±

```python
def compute_loss(self, model, inputs, ...):
    # 1. è®¡ç®—å½“å‰ç­–ç•¥çš„ log æ¦‚ç‡
    per_token_logps = self._get_per_token_logps(model, input_ids, ...)
    
    # 2. è®¡ç®—å‚è€ƒæ¨¡å‹çš„ log æ¦‚ç‡ï¼ˆå·²åœ¨å‰å‘ä¼ æ’­ä¸­è®¡ç®—ï¼‰
    ref_per_token_logps = inputs["ref_per_token_logps"]
    
    # 3. è®¡ç®— KL æ•£åº¦
    per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - \
                   (ref_per_token_logps - per_token_logps) - 1
    
    # 4. è®¡ç®—ä¼˜åŠ¿åŠ æƒæŸå¤±
    per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * \
                     advantages.unsqueeze(1)
    
    # 5. æ·»åŠ  KL æƒ©ç½š
    per_token_loss = -(per_token_loss - self.beta * per_token_kl)
    
    # 6. å¹³å‡æŸå¤±
    loss = ((per_token_loss * completion_mask).sum(dim=1) / 
            completion_mask.sum(dim=1)).mean()
    
    return loss
```

**æŸå¤±å‡½æ•°ç»„æˆ**ï¼š
- **ç­–ç•¥æŸå¤±**ï¼š`exp(logp - logp.detach()) * advantages`
- **KL æƒ©ç½š**ï¼š`beta * KL(Ï€_Î¸ || Ï€_ref)`
- **æ€»æŸå¤±**ï¼š`-(ç­–ç•¥æŸå¤± - KLæƒ©ç½š)`

## ğŸ å¥–åŠ±å‡½æ•°è®¾è®¡

### 1. Rule Rewardï¼ˆè§„åˆ™å¥–åŠ±ï¼‰

```python
def rule_reward(prompts, completions):
    """äºŒå…ƒå¥–åŠ±ï¼šæ­£ç¡®=1.0ï¼Œé”™è¯¯=0.0"""
    history = [prompt2history[prompt] for prompt in prompts]
    targets = [history2target[elm] for elm in history]
    rewards = []
    
    for i, completion in enumerate(completions):
        if completion.strip("\n\" ") == targets[i].strip("\n\" "):
            rewards.append(1.0)  # å®Œå…¨åŒ¹é…
        else:
            rewards.append(0.0)  # ä¸åŒ¹é…
    
    return rewards
```

**ç‰¹ç‚¹**ï¼š
- ç®€å•ç›´æ¥
- åªå¥–åŠ±å®Œå…¨æ­£ç¡®çš„æ¨è
- é€‚åˆç²¾ç¡®åŒ¹é…åœºæ™¯

### 2. NDCG Rule Rewardï¼ˆæ’åºæ„ŸçŸ¥å¥–åŠ±ï¼‰

```python
def ndcg_rule_reward(prompts, completions):
    """åŸºäºä½ç½®çš„å¥–åŠ±ï¼šè€ƒè™‘æ’åºä½ç½®"""
    # è®¡ç®— NDCG é£æ ¼çš„å¥–åŠ±æƒé‡
    ndcg_rewards = [-1.0/math.log2(i+2) for i in range(num_generations)]
    ndcg_rewards = [-elm/sum(ndcg_rewards) for elm in ndcg_rewards]
    
    rewards = []
    for i, completion in enumerate(completions):
        if completion.strip("\n\"") == targets[i].strip("\n\""):
            lis.append(0.0)  # æ­£ç¡®ç­”æ¡ˆï¼Œæ— æƒ©ç½š
        else:
            # æ ¹æ®ä½ç½®ç»™äºˆè´Ÿå¥–åŠ±
            lis.append(ndcg_rewards[i % num_generations])
    
    # å¦‚æœç»„å†…æ²¡æœ‰æ­£ç¡®ç­”æ¡ˆï¼Œå…¨éƒ¨ç»™ 0
    if flag:  # ç»„å†…æœ‰æ­£ç¡®ç­”æ¡ˆ
        rewards.extend(lis)
    else:
        rewards.extend([0.0] * num_generations)
    
    return rewards
```

**ç‰¹ç‚¹**ï¼š
- è€ƒè™‘æ¨èä½ç½®
- ä½ç½®è¶Šé å‰ï¼Œè´Ÿå¥–åŠ±è¶Šå¤§
- é¼“åŠ±æ¨¡å‹å°†æ­£ç¡®ç­”æ¡ˆæ’åœ¨å‰é¢

### 3. Semantic Rewardï¼ˆè¯­ä¹‰å¥–åŠ±ï¼‰

```python
def semantic_reward(prompts, completions):
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„å¥–åŠ±"""
    target_ids = [item2id[target.strip("\"\n")] for target in targets]
    completion_ids = [item2id[completion.strip("\"\n")] for completion in completions]
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    rewards = torch.cosine_similarity(
        item_ada_embd[target_ids],
        item_ada_embd[completion_ids],
        dim=-1
    )
    
    return rewards
```

**ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨å•†å“åµŒå…¥è®¡ç®—ç›¸ä¼¼åº¦
- å¥–åŠ±è¯­ä¹‰ç›¸è¿‘çš„å•†å“
- é€‚åˆå¤šæ ·æ€§æ¨èåœºæ™¯

### 4. CF Rewardï¼ˆååŒè¿‡æ»¤å¥–åŠ±ï¼‰

```python
def cf_reward(prompts, completions):
    """åŸºäº SASRec æ¨¡å‹çš„å¥–åŠ±"""
    # ä½¿ç”¨é¢„è®­ç»ƒçš„ SASRec æ¨¡å‹
    history_ids = [item2id[elm] for elm in history_list]
    pred_ids = [item2id[completion.strip("\n\"")] for completion in completions]
    
    # é€šè¿‡ SASRec é¢„æµ‹åˆ†æ•°
    with torch.no_grad():
        predictions = model.forward_eval(seq, lengths)
        scores = torch.gather(predictions, 1, pred.view(-1, 1)).view(-1)
    
    return scores
```

**ç‰¹ç‚¹**ï¼š
- åˆ©ç”¨ååŒè¿‡æ»¤ä¿¡å·
- ç»“åˆä¼ ç»Ÿæ¨èæ¨¡å‹
- æä¾›æ›´ä¸°å¯Œçš„å¥–åŠ±ä¿¡å·

### å¥–åŠ±å‡½æ•°ç»„åˆ

```python
if reward_type == "rule":
    reward_fun = rule_reward
elif reward_type == "ranking":
    reward_fun = [rule_reward, ndcg_rule_reward]  # ç»„åˆå¥–åŠ±
elif reward_type == "ranking_only":
    reward_fun = ndcg_rule_reward
elif reward_type == "semantic":
    reward_fun = semantic_reward
elif reward_type == "sasrec":
    reward_fun = cf_reward
```

## ğŸ”’ çº¦æŸè§£ç 

### ConstrainedLogitsProcessor

```python
class ConstrainedLogitsProcessor(LogitsProcessor):
    """
    ç¡®ä¿ç”Ÿæˆçš„ token ç¬¦åˆ SID æ ¼å¼
    """
    def __call__(self, input_ids, scores):
        # 1. è·å–å½“å‰å‰ç¼€
        hash_key = input_ids[-self.count:].tolist()
        
        # 2. æŸ¥æ‰¾å…è®¸çš„ token
        prefix_allowed_tokens = self._prefix_allowed_tokens_fn(batch_id, hash_key)
        
        # 3. æ©ç ä¸å…è®¸çš„ token
        mask = torch.full_like(scores, -1000000)
        mask[prefix_allowed_tokens] = 0
        
        # 4. åº”ç”¨æ©ç 
        scores = scores + mask
        
        return scores
```

**å·¥ä½œåŸç†**ï¼š
1. æ ¹æ®å½“å‰ç”Ÿæˆçš„å‰ç¼€æŸ¥æ‰¾æœ‰æ•ˆ token
2. ä½¿ç”¨å‰ç¼€å“ˆå¸Œè¡¨å¿«é€ŸæŸ¥æ‰¾
3. å°†æ— æ•ˆ token çš„ logits è®¾ä¸ºæå°å€¼
4. ç¡®ä¿ç”Ÿæˆçš„åºåˆ—ç¬¦åˆ SID æ ¼å¼

## ğŸ›ï¸ é«˜çº§ç‰¹æ€§

### 1. åŠ¨æ€é‡‡æ ·ï¼ˆDynamic Samplingï¼‰

```python
if dynamic_sampling:
    # ç”Ÿæˆæ›´å¤šå€™é€‰ï¼ˆ1.5å€ï¼‰
    extended_prompt_ids = torch.stack([prompt_ids[i]] * int(1.5 * num_generations))
    
    # ç”Ÿæˆå€™é€‰
    extended_completions = model.generate(...)
    
    # é€‰æ‹©æœ€ä½³å€™é€‰
    def select_completion(completions, target):
        # ä¼˜å…ˆé€‰æ‹©åŒ…å«æ­£ç¡®ç­”æ¡ˆçš„å€™é€‰
        # ç„¶åé€‰æ‹©å‡ºç°é¢‘ç‡é«˜çš„å€™é€‰
        ...
    
    selected_completion = select_completion(extended_completions, target)
```

**ä¼˜åŠ¿**ï¼š
- å¢åŠ å€™é€‰å¤šæ ·æ€§
- æé«˜æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡
- è‡ªé€‚åº”é€‰æ‹©ç­–ç•¥

### 2. æŸæœç´¢ï¼ˆBeam Searchï¼‰

```python
if beam_search:
    self.generation_config = GenerationConfig(
        max_new_tokens=self.max_completion_length,
        num_beams=self.num_generations,          # æŸå¤§å° = ç”Ÿæˆæ•°é‡
        num_return_sequences=self.num_generations,
        length_penalty=self.length_penalty,
    )
```

**ä¼˜åŠ¿**ï¼š
- æ›´é«˜è´¨é‡çš„å€™é€‰
- ä¿è¯å€™é€‰å¤šæ ·æ€§
- é€‚åˆç²¾ç¡®æ¨èåœºæ™¯

### 3. è®­ç»ƒä¸­æµ‹è¯•ï¼ˆTest During Trainingï¼‰

```python
if test_during_training:
    # åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•
    test_completion_ids = model.generate(
        dedup_prompt_ids,
        generation_config=self.test_generation_config,  # num_beams=20
        logits_processor=self.test_lp_list,
    )
    
    # è®¡ç®— HR@K å’Œ NDCG@K
    for i, comp_lis in enumerate(test_comp_lis):
        target = dedup_target[i]
        for j in range(len(comp_lis)):
            if comp_lis[j].strip("\n\"") == target.strip("\n\""):
                # è®¡ç®—æŒ‡æ ‡
                hr[index] += 1
                ndcg[index] += 1 / math.log2(j+2)
```

**ä¼˜åŠ¿**ï¼š
- å®æ—¶ç›‘æ§æ¨¡å‹æ€§èƒ½
- åŠæ—¶å‘ç°è¿‡æ‹Ÿåˆ
- æŒ‡å¯¼è®­ç»ƒç­–ç•¥è°ƒæ•´

## ğŸ“Š è®­ç»ƒé…ç½®

### å…³é”®è¶…å‚æ•°

```python
training_args = GRPOConfig(
    output_dir=output_dir,
    max_completion_length=128,          # æœ€å¤§ç”Ÿæˆé•¿åº¦
    num_generations=16,                  # æ¯ä¸ª prompt ç”Ÿæˆæ•°é‡
    temperature=1.0,                     # é‡‡æ ·æ¸©åº¦
    beta=0.04,                           # KL æƒ©ç½šç³»æ•°
    learning_rate=1e-6,                  # å­¦ä¹ ç‡ï¼ˆè¾ƒå°ï¼‰
    num_train_epochs=1,                  # è®­ç»ƒè½®æ•°
    gradient_accumulation_steps=2,       # æ¢¯åº¦ç´¯ç§¯
    max_grad_norm=0.3,                   # æ¢¯åº¦è£å‰ª
    sync_ref_model=True,                 # åŒæ­¥å‚è€ƒæ¨¡å‹
)
```

### è®­ç»ƒç­–ç•¥

1. **å°å­¦ä¹ ç‡**ï¼šRL é˜¶æ®µä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆ1e-6ï¼‰ï¼Œé¿å…ç ´å SFT æ¨¡å‹
2. **KL æƒ©ç½š**ï¼šé€šè¿‡ beta å‚æ•°æ§åˆ¶ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ç¨‹åº¦
3. **æ¢¯åº¦ç´¯ç§¯**ï¼šä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å¤„ç†å¤§æ‰¹æ¬¡
4. **å‚è€ƒæ¨¡å‹åŒæ­¥**ï¼šå®šæœŸåŒæ­¥å‚è€ƒæ¨¡å‹ï¼Œä¿æŒç¨³å®šæ€§

## ğŸ”„ å®Œæ•´è®­ç»ƒå¾ªç¯

```python
for epoch in range(num_train_epochs):
    for batch in train_dataloader:
        # 1. ç”Ÿæˆå€™é€‰
        completions = model.generate(prompts, num_generations=16)
        
        # 2. è®¡ç®—å¥–åŠ±
        rewards = reward_func(prompts, completions)
        
        # 3. ç»„å†…å½’ä¸€åŒ–
        advantages = normalize_rewards(rewards, num_generations=16)
        
        # 4. è®¡ç®—æŸå¤±
        loss = compute_grpo_loss(
            model_logps,      # å½“å‰ç­–ç•¥ log æ¦‚ç‡
            ref_logps,        # å‚è€ƒæ¨¡å‹ log æ¦‚ç‡
            advantages,       # ä¼˜åŠ¿å‡½æ•°
            beta=0.04         # KL æƒ©ç½šç³»æ•°
        )
        
        # 5. åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # 6. åŒæ­¥å‚è€ƒæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        if sync_ref_model:
            sync_reference_model()
```

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡

è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§ä»¥ä¸‹æŒ‡æ ‡ï¼š

- **reward**ï¼šå¹³å‡å¥–åŠ±
- **reward_std**ï¼šå¥–åŠ±æ ‡å‡†å·®
- **kl**ï¼šKL æ•£åº¦ï¼ˆä¸å‚è€ƒæ¨¡å‹çš„å·®å¼‚ï¼‰
- **categorical_diversity**ï¼šç±»åˆ«å¤šæ ·æ€§
- **token_diversity**ï¼šToken å¤šæ ·æ€§
- **HR@K**ï¼šå‘½ä¸­ç‡ï¼ˆå¦‚æœå¯ç”¨æµ‹è¯•ï¼‰
- **NDCG@K**ï¼šå½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```bash
bash rl.sh
```

### è‡ªå®šä¹‰é…ç½®

```python
python rl.py \
    --model_path ./output/sft/final_checkpoint \
    --train_file ./data/train.csv \
    --eval_file ./data/valid.csv \
    --info_file ./data/info.txt \
    --category Industrial_and_Scientific \
    --reward_type ranking \
    --num_generations 16 \
    --beam_search True \
    --beta 0.04 \
    --learning_rate 1e-6 \
    --num_train_epochs 2
```

## ğŸ’¡ å…³é”®è®¾è®¡æ€æƒ³

1. **ç»„å†…å½’ä¸€åŒ–**ï¼šæé«˜è®­ç»ƒç¨³å®šæ€§
2. **çº¦æŸè§£ç **ï¼šç¡®ä¿ç”Ÿæˆæœ‰æ•ˆ SID
3. **å¤šå¥–åŠ±å‡½æ•°**ï¼šçµæ´»ç»„åˆä¸åŒå¥–åŠ±ä¿¡å·
4. **KL æƒ©ç½š**ï¼šé˜²æ­¢ç­–ç•¥è¿‡åº¦åç¦»
5. **åŠ¨æ€é‡‡æ ·**ï¼šæé«˜å€™é€‰è´¨é‡

## ğŸ”— ç›¸å…³è®ºæ–‡

- **GRPO**ï¼šDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
- **MiniOneRec**ï¼šAn Open-Source Framework for Scaling Generative Recommendation

## ğŸ“ æ€»ç»“

MiniOneRec çš„å¼ºåŒ–å­¦ä¹ å®ç°ï¼š

1. **ç®—æ³•**ï¼šåŸºäº GRPOï¼Œé€‚é…æ¨èåœºæ™¯
2. **å¥–åŠ±**ï¼šæ”¯æŒå¤šç§å¥–åŠ±å‡½æ•°ï¼ˆè§„åˆ™ã€æ’åºã€è¯­ä¹‰ã€CFï¼‰
3. **è§£ç **ï¼šçº¦æŸè§£ç ç¡®ä¿ç”Ÿæˆæœ‰æ•ˆ SID
4. **è®­ç»ƒ**ï¼šç»„å†…å½’ä¸€åŒ– + KL æƒ©ç½šï¼Œç¨³å®šé«˜æ•ˆ
5. **ç‰¹æ€§**ï¼šåŠ¨æ€é‡‡æ ·ã€æŸæœç´¢ã€è®­ç»ƒä¸­æµ‹è¯•

è¯¥å®ç°ä¸ºç”Ÿæˆå¼æ¨èç³»ç»Ÿæä¾›äº†å®Œæ•´çš„ RL è®­ç»ƒè§£å†³æ–¹æ¡ˆã€‚

