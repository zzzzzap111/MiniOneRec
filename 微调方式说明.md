# MiniOneRec å¾®è°ƒæ–¹å¼è¯´æ˜

## ğŸ“‹ æ ¸å¿ƒç»“è®º

**MiniOneRec é»˜è®¤ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-tuningï¼‰ï¼Œä¸æ˜¯ LoRA æˆ– QLoRAã€‚**

ä½†ä»£ç **æ”¯æŒ** LoRAï¼Œå¯ä»¥é€šè¿‡é…ç½®å¯ç”¨ã€‚

## ğŸ” è¯¦ç»†åˆ†æ

### 1. SFT é˜¶æ®µï¼ˆç›‘ç£å¾®è°ƒï¼‰

#### é»˜è®¤æ–¹å¼ï¼šå…¨å‚æ•°å¾®è°ƒ

```python
# sft.py ä¸­çš„æ¨¡å‹åŠ è½½
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.bfloat16,  # æ··åˆç²¾åº¦ï¼Œä¸æ˜¯é‡åŒ–
)
```

**ç‰¹ç‚¹**ï¼š
- âœ… **å…¨å‚æ•°è®­ç»ƒ**ï¼šæ‰€æœ‰æ¨¡å‹å‚æ•°éƒ½ä¼šæ›´æ–°
- âœ… **æ··åˆç²¾åº¦**ï¼šä½¿ç”¨ `bfloat16` èŠ‚çœæ˜¾å­˜
- âŒ **ä¸ä½¿ç”¨ LoRA**ï¼šæ²¡æœ‰ `get_peft_model` è°ƒç”¨
- âŒ **ä¸ä½¿ç”¨é‡åŒ–**ï¼šæ²¡æœ‰ `load_in_4bit` æˆ– `load_in_8bit`

#### å¯é€‰æ–¹å¼ï¼šå†»ç»“ LLM å‚æ•°

```python
# å¦‚æœè®¾ç½® freeze_LLM=True
if freeze_LLM:
    # å†»ç»“æ‰€æœ‰ LLM å‚æ•°
    for param in model.parameters():
        param.requires_grad = False
    
    # åªè®­ç»ƒæ–°æ·»åŠ çš„ SID token embeddings
    embedding_layer.weight.requires_grad = True
```

**ç‰¹ç‚¹**ï¼š
- å†»ç»“åŸå§‹ LLM å‚æ•°
- åªè®­ç»ƒæ–°æ·»åŠ çš„ SID token embeddings
- ç±»ä¼¼ LoRA çš„å‚æ•°æ•ˆç‡ï¼Œä½†åªé’ˆå¯¹ embedding å±‚

### 2. RL é˜¶æ®µï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰

#### é»˜è®¤æ–¹å¼ï¼šå…¨å‚æ•°å¾®è°ƒ

```python
# rl.py ä¸­çš„æ¨¡å‹åŠ è½½
llm_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
```

**ç‰¹ç‚¹**ï¼š
- âœ… **å…¨å‚æ•°è®­ç»ƒ**ï¼šæ‰€æœ‰å‚æ•°éƒ½ä¼šæ›´æ–°
- âœ… **ä½¿ç”¨ bitsandbytes ä¼˜åŒ–å™¨**ï¼š`optim="paged_adamw_32bit"`ï¼ˆèŠ‚çœæ˜¾å­˜ï¼Œä½†ä¸æ˜¯é‡åŒ–ï¼‰
- âŒ **ä¸ä½¿ç”¨ LoRA**

**æ³¨æ„**ï¼š`paged_adamw_32bit` æ˜¯ bitsandbytes æä¾›çš„ä¼˜åŒ–å™¨ï¼Œç”¨äºï¼š
- åˆ†é¡µå†…å­˜ç®¡ç†ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
- 32ä½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆç›¸æ¯” 64ä½èŠ‚çœå†…å­˜ï¼‰
- **ä¸æ˜¯æ¨¡å‹é‡åŒ–**

### 3. ä»£ç ä¸­çš„ LoRA æ”¯æŒ

è™½ç„¶é»˜è®¤ä¸ä½¿ç”¨ï¼Œä½†ä»£ç **æ”¯æŒ** LoRAï¼š

```python
# minionerec_trainer.py
from transformers.utils import is_peft_available
if is_peft_available():
    from peft import PeftConfig, get_peft_model

class ReReTrainer(Trainer):
    def __init__(
        self,
        ...
        peft_config: Optional["PeftConfig"] = None,  # æ”¯æŒä¼ å…¥ LoRA é…ç½®
    ):
        ...
        if peft_config is not None:
            model = get_peft_model(model, peft_config)  # å¯ä»¥å¯ç”¨ LoRA
```

**ä½†å®é™…ä½¿ç”¨ä¸­**ï¼š
- `rl.py` è°ƒç”¨ `ReReTrainer` æ—¶**æ²¡æœ‰ä¼ å…¥** `peft_config`
- `sft.py` ä½¿ç”¨æ ‡å‡† `Trainer`ï¼Œ**ä¸æ”¯æŒ** LoRA

## ğŸ“Š å¯¹æ¯”æ€»ç»“

| å¾®è°ƒæ–¹å¼ | æ˜¯å¦ä½¿ç”¨ | è¯´æ˜ |
|---------|---------|------|
| **å…¨å‚æ•°å¾®è°ƒ** | âœ… **é»˜è®¤** | æ‰€æœ‰å‚æ•°éƒ½è®­ç»ƒ |
| **LoRA** | âš ï¸ **æ”¯æŒä½†æœªå¯ç”¨** | ä»£ç æ”¯æŒï¼Œä½†é»˜è®¤ä¸ä½¿ç”¨ |
| **QLoRA** | âŒ **ä¸æ”¯æŒ** | æ²¡æœ‰é‡åŒ–é…ç½® |
| **å†»ç»“ Embedding** | âœ… **å¯é€‰** | `freeze_LLM=True` æ—¶åªè®­ç»ƒæ–° token |

## ğŸ”§ å¦‚ä½•å¯ç”¨ LoRAï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æƒ³ä½¿ç”¨ LoRA æ¥èŠ‚çœæ˜¾å­˜ï¼Œå¯ä»¥ä¿®æ”¹ä»£ç ï¼š

### ä¿®æ”¹ SFT è„šæœ¬

```python
# åœ¨ sft.py ä¸­æ·»åŠ 
from peft import LoraConfig, get_peft_model

# åˆ›å»º LoRA é…ç½®
lora_config = LoraConfig(
    r=16,                    # LoRA rank
    lora_alpha=32,           # LoRA alpha
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # ç›®æ ‡æ¨¡å—
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# åº”ç”¨ LoRA
model = get_peft_model(model, lora_config)
```

### ä¿®æ”¹ RL è„šæœ¬

```python
# åœ¨ rl.py ä¸­ä¿®æ”¹
from peft import LoraConfig

lora_config = LoraConfig(...)

trainer = ReReTrainer(
    model=model_path,
    peft_config=lora_config,  # ä¼ å…¥ LoRA é…ç½®
    ...
)
```

## ğŸ’¾ æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥

è™½ç„¶ä¸ä½¿ç”¨ LoRA/QLoRAï¼Œä½†é¡¹ç›®ä½¿ç”¨äº†å…¶ä»–æ˜¾å­˜ä¼˜åŒ–æ–¹æ³•ï¼š

### 1. æ··åˆç²¾åº¦è®­ç»ƒ

```python
# SFT
bf16=True  # æˆ– torch_dtype=torch.bfloat16

# RL
bf16=True
```

### 2. DeepSpeed ZeRO-2

```yaml
# config/zero2_opt.yaml
deepspeed_config:
  zero_stage: 2  # ZeRO-2 ä¼˜åŒ–
  offload_optimizer_device: none
  offload_param_device: none
```

**ZeRO-2 çš„ä½œç”¨**ï¼š
- åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€
- åˆ†ç‰‡æ¢¯åº¦
- åˆ†ç‰‡å‚æ•°ï¼ˆå¯é€‰ï¼‰
- æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨

### 3. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰

```python
# åœ¨è®­ç»ƒå‚æ•°ä¸­å¯ç”¨
--enable_gradient_checkpointing
```

**ä½œç”¨**ï¼šç”¨è®¡ç®—æ—¶é—´æ¢æ˜¾å­˜ç©ºé—´

### 4. BitsAndBytes ä¼˜åŒ–å™¨

```python
# RL é˜¶æ®µä½¿ç”¨
optim="paged_adamw_32bit"  # åˆ†é¡µ + 32ä½ä¼˜åŒ–å™¨
```

**ä½œç”¨**ï¼š
- åˆ†é¡µå†…å­˜ç®¡ç†
- 32ä½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆèŠ‚çœ 50% æ˜¾å­˜ï¼‰

## ğŸ“ˆ æ˜¾å­˜éœ€æ±‚å¯¹æ¯”

### å…¨å‚æ•°å¾®è°ƒï¼ˆé»˜è®¤ï¼‰

| æ¨¡å‹å¤§å° | å•å¡æ˜¾å­˜éœ€æ±‚ | æ¨èé…ç½® |
|---------|------------|---------|
| Qwen2-0.5B | 2-4GB | å•å¡è®­ç»ƒ |
| Qwen2-1.5B | 4-8GB | å•å¡è®­ç»ƒ |
| Qwen2-3B | 10-16GB | å•å¡æˆ– 2å¡ |
| Qwen2-7B | 20-32GB | å¤šå¡ï¼ˆ4-8å¡ï¼‰ |

### å¦‚æœä½¿ç”¨ LoRAï¼ˆç†è®ºå€¼ï¼‰

| æ¨¡å‹å¤§å° | LoRA æ˜¾å­˜éœ€æ±‚ | èŠ‚çœæ¯”ä¾‹ |
|---------|-------------|---------|
| Qwen2-7B | 8-12GB | ~60% |
| Qwen2-14B | 16-24GB | ~60% |

## ğŸ¯ ä¸ºä»€ä¹ˆé»˜è®¤ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒï¼Ÿ

### ä¼˜åŠ¿

1. **æ€§èƒ½æ›´å¥½**ï¼šå…¨å‚æ•°å¾®è°ƒé€šå¸¸æ¯” LoRA æ€§èƒ½æ›´å¥½
2. **ç®€å•ç›´æ¥**ï¼šä¸éœ€è¦é…ç½® LoRA å‚æ•°
3. **é€‚åˆæ¨èåœºæ™¯**ï¼šæ¨èä»»åŠ¡éœ€è¦æ¨¡å‹å……åˆ†é€‚åº”æ–°é¢†åŸŸ

### åŠ£åŠ¿

1. **æ˜¾å­˜éœ€æ±‚å¤§**ï¼šéœ€è¦æ›´å¤š GPU èµ„æº
2. **è®­ç»ƒæ—¶é—´é•¿**ï¼šéœ€è¦æ›´æ–°æ‰€æœ‰å‚æ•°

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### åœºæ™¯ 1ï¼šèµ„æºå……è¶³ï¼ˆæ¨èï¼‰

**ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒ**ï¼ˆé»˜è®¤æ–¹å¼ï¼‰
- æ€§èƒ½æœ€ä½³
- ä»£ç ç®€å•
- é€‚åˆç”Ÿäº§ç¯å¢ƒ

### åœºæ™¯ 2ï¼šèµ„æºå—é™

**é€‰é¡¹ Aï¼šä½¿ç”¨ DeepSpeed ZeRO**
```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶
accelerate launch --config_file ./config/zero2_opt.yaml ...
```

**é€‰é¡¹ Bï¼šå¯ç”¨ LoRA**ï¼ˆéœ€è¦ä¿®æ”¹ä»£ç ï¼‰
- ä¿®æ”¹ `sft.py` æ·»åŠ  LoRA é…ç½®
- ä¿®æ”¹ `rl.py` ä¼ å…¥ `peft_config`

**é€‰é¡¹ Cï¼šå†»ç»“ LLM å‚æ•°**
```bash
python sft.py \
    --freeze_LLM True \  # åªè®­ç»ƒæ–° token embeddings
    ...
```

### åœºæ™¯ 3ï¼šè¶…å¤§æ¨¡å‹ï¼ˆ> 7Bï¼‰

**æ¨èç»„åˆ**ï¼š
- DeepSpeed ZeRO-3ï¼ˆåˆ†ç‰‡å‚æ•°ï¼‰
- æ¢¯åº¦æ£€æŸ¥ç‚¹
- æ··åˆç²¾åº¦è®­ç»ƒ
- æˆ–ä½¿ç”¨ LoRAï¼ˆéœ€è¦ä¿®æ”¹ä»£ç ï¼‰

## ğŸ” ä»£ç è¯æ®

### è¯æ® 1ï¼šSFT ä¸ä½¿ç”¨ LoRA

```python
# sft.py line 135-138
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.bfloat16,  # åªæœ‰æ··åˆç²¾åº¦ï¼Œæ²¡æœ‰é‡åŒ–
)
# æ²¡æœ‰ get_peft_model è°ƒç”¨
```

### è¯æ® 2ï¼šRL ä¸ä½¿ç”¨ LoRA

```python
# rl.py line 288-305
trainer = ReReTrainer(
    model=model_path,
    base_model=model_path,
    # peft_config=None,  # æ²¡æœ‰ä¼ å…¥ LoRA é…ç½®
    ...
)
```

### è¯æ® 3ï¼šæ”¯æŒä½†æœªå¯ç”¨

```python
# minionerec_trainer.py line 280-281
if peft_config is not None:  # æ”¯æŒï¼Œä½†é»˜è®¤æ˜¯ None
    model = get_peft_model(model, peft_config)
```

### è¯æ® 4ï¼šBitsAndBytes çš„ç”¨é€”

```python
# requirements.txt
bitsandbytes==0.48.1  # ç”¨äºä¼˜åŒ–å™¨ï¼Œä¸æ˜¯é‡åŒ–

# rl.py
optim="paged_adamw_32bit"  # bitsandbytes ä¼˜åŒ–å™¨ï¼Œä¸æ˜¯é‡åŒ–
```

## ğŸ“ æ€»ç»“

| é—®é¢˜ | ç­”æ¡ˆ |
|------|------|
| **ä½¿ç”¨ LoRA å—ï¼Ÿ** | âŒ é»˜è®¤ä¸ä½¿ç”¨ï¼Œä½†ä»£ç æ”¯æŒ |
| **ä½¿ç”¨ QLoRA å—ï¼Ÿ** | âŒ ä¸æ”¯æŒ |
| **ä½¿ç”¨ä»€ä¹ˆæ–¹å¼ï¼Ÿ** | âœ… **å…¨å‚æ•°å¾®è°ƒ**ï¼ˆFull Fine-tuningï¼‰ |
| **æ˜¾å­˜ä¼˜åŒ–ï¼Ÿ** | âœ… DeepSpeed ZeRO-2 + æ··åˆç²¾åº¦ + ä¼˜åŒ–å™¨ä¼˜åŒ– |
| **å¯ä»¥å¯ç”¨ LoRA å—ï¼Ÿ** | âœ… å¯ä»¥ï¼Œéœ€è¦ä¿®æ”¹ä»£ç  |

**å…³é”®ç‚¹**ï¼š
- é»˜è®¤ï¼š**å…¨å‚æ•°å¾®è°ƒ** + **æ··åˆç²¾åº¦** + **DeepSpeed ZeRO-2**
- å¯é€‰ï¼š**å†»ç»“ LLM**ï¼ˆåªè®­ç»ƒæ–° token embeddingsï¼‰
- æ”¯æŒï¼š**LoRA**ï¼ˆéœ€è¦ä¿®æ”¹ä»£ç å¯ç”¨ï¼‰
- ä¸æ”¯æŒï¼š**QLoRA**ï¼ˆé‡åŒ– + LoRAï¼‰

