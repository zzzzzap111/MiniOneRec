# MiniOneRec LoRA å¾®è°ƒè¯¦ç»†æ­¥éª¤ï¼ˆA100 äº‘æœåŠ¡å™¨ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆå®Œæ•´æµç¨‹ï¼‰

### å½“å‰çŠ¶æ€ç¡®è®¤

æ‚¨çš„æƒ…å†µï¼š
- âœ… äº‘æœåŠ¡å™¨ï¼šA100 GPU
- âœ… æ¨¡å‹ï¼šQwen2.5-3B-Instructï¼ˆå·²ä¸‹è½½ï¼‰
- âœ… ç¯å¢ƒï¼šå·²é…ç½®ï¼ˆ`pip install -r requirements.txt`ï¼‰
- âœ… å¾®è°ƒæ–¹å¼ï¼šLoRA
- âœ… ä»£ç ï¼šå·²ä¿®æ”¹æ”¯æŒ LoRA

### å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆ4æ­¥ï¼‰

**æ­¥éª¤ 0ï¼šä¿®æ”¹ä»£ç æ”¯æŒ LoRAï¼ˆ5åˆ†é’Ÿï¼Œåªéœ€ä¸€æ¬¡ï¼‰**

âš ï¸ **é‡è¦**ï¼šåŸå§‹ä»£ç ä¸æ”¯æŒ LoRAï¼Œéœ€è¦å…ˆä¿®æ”¹ `sft.py` å’Œ `rl.py`ã€‚

**å¿«é€Ÿä¿®æ”¹æ–¹æ³•**ï¼š

1. åœ¨ `sft.py` æ–‡ä»¶å¼€å¤´ï¼ˆç¬¬ 23 è¡Œé™„è¿‘ï¼‰æ·»åŠ å¯¼å…¥ï¼š
```python
from peft import LoraConfig, get_peft_model, TaskType
```

2. åœ¨ `sft.py` çš„ `train()` å‡½æ•°å‚æ•°ä¸­ï¼ˆç¬¬ 108 è¡Œé™„è¿‘ï¼‰æ·»åŠ  LoRA å‚æ•°ï¼š
```python
def train(
    # ... ç°æœ‰å‚æ•° ...
    freeze_LLM: bool = False,
    # LoRA paramsï¼ˆæ–°å¢ä»¥ä¸‹5è¡Œï¼‰
    use_lora: bool = False,
    lora_r: int = 16,
    lora_alpha: int = 32,
    lora_dropout: float = 0.05,
    lora_target_modules: str = "all",
    # wandb params
    wandb_project: str = "",
    # ... å…¶ä»–å‚æ•° ...
):
```

3. åœ¨ `sft.py` çš„æ¨¡å‹åŠ è½½åï¼ˆç¬¬ 166 è¡Œé™„è¿‘ï¼Œ`model.resize_token_embeddings(len(tokenizer))` ä¹‹åï¼‰æ·»åŠ  LoRA é…ç½®ï¼š
```python
    # ========== æ·»åŠ  LoRA é…ç½® ==========
    if use_lora:
        print("=" * 50)
        print("å¯ç”¨ LoRA å¾®è°ƒ")
        print("=" * 50)
        
        # ç¡®å®šç›®æ ‡æ¨¡å—
        if lora_target_modules == "all":
            if hasattr(model, 'model'):
                model_base = model.model
            else:
                model_base = model
            
            target_modules = []
            for name, module in model_base.named_modules():
                if any(x in name for x in ["q_proj", "k_proj", "v_proj", "o_proj", 
                                            "gate_proj", "up_proj", "down_proj"]):
                    target_modules.append(name.split('.')[-1])
            
            target_modules = list(set(target_modules))
            if not target_modules:
                target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        elif lora_target_modules == "qkv":
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        else:
            target_modules = [m.strip() for m in lora_target_modules.split(",")]
        
        print(f"LoRA ç›®æ ‡æ¨¡å—: {target_modules}")
        
        # åˆ›å»º LoRA é…ç½®
        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        # åº”ç”¨ LoRA
        model = get_peft_model(model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} "
              f"({100*trainable_params/total_params:.4f}%)")
        print("=" * 50)
```

4. åŒæ ·ä¿®æ”¹ `rl.py`ï¼ˆå‚è€ƒä¸‹é¢çš„"è¯¦ç»†ä»£ç ä¿®æ”¹"éƒ¨åˆ†ï¼‰

**æˆ–è€…ä½¿ç”¨æˆ‘æä¾›çš„ä¿®æ”¹ç¤ºä¾‹**ï¼š
- å‚è€ƒ `LoRA_ä»£ç ä¿®æ”¹ç¤ºä¾‹.py` æ–‡ä»¶
- æˆ–æŸ¥çœ‹ä¸‹é¢çš„"æ­¥éª¤ 3ï¼šä¿®æ”¹ä»£ç ä»¥æ”¯æŒ LoRA"éƒ¨åˆ†

**æ­¥éª¤ 1ï¼šç¡®è®¤æ•°æ®å’Œæ¨¡å‹ä½ç½®ï¼ˆ5åˆ†é’Ÿï¼‰**

```bash
# 1. è¿›å…¥é¡¹ç›®ç›®å½•
cd /path/to/MiniOneRec

# 2. æ£€æŸ¥æ•°æ®æ–‡ä»¶
ls -lh ./data/Amazon/train/Industrial_and_Scientific*
ls -lh ./data/Amazon/valid/Industrial_and_Scientific*
ls -lh ./data/Amazon/index/Industrial_and_Scientific*

# 3. ç¡®è®¤æ¨¡å‹ä½ç½®
# å¦‚æœæ¨¡å‹åœ¨é»˜è®¤ä½ç½®ï¼š~/.cache/huggingface/hub/
# ç›´æ¥ä½¿ç”¨ï¼šQwen/Qwen2.5-3B-Instruct
# å¦‚æœæ¨¡å‹åœ¨è‡ªå®šä¹‰ç›®å½•ï¼š./models/Qwen2.5-3B-Instruct
# ä½¿ç”¨å®Œæ•´è·¯å¾„
```

**æ­¥éª¤ 2ï¼šè¿è¡Œ SFT è®­ç»ƒï¼ˆ2-4å°æ—¶ï¼‰**

```bash
python sft.py \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --train_file ./data/Amazon/train/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --eval_file ./data/Amazon/valid/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --output_dir ./output/sft_lora_qwen25_3b \
    --use_lora True \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --lora_target_modules "all" \
    --batch_size 1024 \
    --micro_batch_size 32 \
    --num_epochs 10 \
    --learning_rate 3e-4 \
    --category Industrial_and_Scientific \
    --sid_index_path ./data/Amazon/index/Industrial_and_Scientific.index.json \
    --item_meta_path ./data/Amazon/index/Industrial_and_Scientific.item.json \
    --wandb_project minionerec_lora \
    --wandb_run_name sft_lora_qwen25_3b
```

**æ­¥éª¤ 3ï¼šè¿è¡Œ RL è®­ç»ƒï¼ˆ1-2å°æ—¶ï¼‰**

```bash
python rl.py \
    --model_path ./output/sft_lora_qwen25_3b/final_checkpoint \
    --train_file ./data/Amazon/train/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --eval_file ./data/Amazon/valid/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --output_dir ./output/rl_lora_qwen25_3b \
    --use_lora True \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --lora_target_modules "all" \
    --info_file ./data/Amazon/info/Industrial_and_Scientific_5_2016-10-2018-11.txt \
    --sid_index_path ./data/Amazon/index/Industrial_and_Scientific.index.json \
    --item_meta_path ./data/Amazon/index/Industrial_and_Scientific.item.json \
    --category Industrial_and_Scientific \
    --train_batch_size 32 \
    --eval_batch_size 32 \
    --num_generations 16 \
    --num_train_epochs 1 \
    --learning_rate 1e-6 \
    --beta 0.04 \
    --reward_type rule \
    --wandb_project minionerec_lora \
    --wandb_run_name rl_lora_qwen25_3b
```

**æ­¥éª¤ 4ï¼šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆ10åˆ†é’Ÿï¼‰**

```bash
python evaluate.py \
    --exp_name ./output/rl_lora_qwen25_3b/final_checkpoint \
    --test_data_path ./data/Amazon/test/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --info_file ./data/Amazon/info/Industrial_and_Scientific_5_2016-10-2018-11.txt \
    --category Industrial_and_Scientific \
    --num_beams 50 \
    --K 10
```

### ç›‘æ§è®­ç»ƒ

åœ¨å¦ä¸€ä¸ªç»ˆç«¯ç›‘æ§GPUä½¿ç”¨ï¼š

```bash
watch -n 1 nvidia-smi
```

### é¢„æœŸè¾“å‡º

**SFTè®­ç»ƒå¼€å§‹æ—¶**ï¼š
```
==================================================
å¯ç”¨ LoRA å¾®è°ƒ
==================================================
LoRA ç›®æ ‡æ¨¡å—: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
å¯è®­ç»ƒå‚æ•°: 13,107,200 / 3,000,000,000 (0.44%)
==================================================
Loading index from ./data/Amazon/index/Industrial_and_Scientific.index.json
Adding 765 new tokens to tokenizer
```

**è®­ç»ƒæ—¶é—´ä¼°ç®—**ï¼š
- SFTè®­ç»ƒï¼šçº¦ 2-4 å°æ—¶ï¼ˆQwen2.5-3B + LoRA + A100ï¼‰
- RLè®­ç»ƒï¼šçº¦ 1-2 å°æ—¶
- æ€»è®¡ï¼šçº¦ 3-6 å°æ—¶

---

## ğŸ“‹ é—®é¢˜å›ç­”

### 1. å½“å‰é¡¹ç›®ç›®å½•ä¸‹æœ‰åŸºç¡€æ¨¡å‹å—ï¼Ÿ

**ç­”æ¡ˆï¼šæ²¡æœ‰**

- é¡¹ç›®ç›®å½•ä¸‹**æ²¡æœ‰åŸºç¡€æ¨¡å‹æ–‡ä»¶**ï¼ˆå¦‚ `.pth`ã€`.bin` ç­‰ï¼‰
- åŸºç¡€æ¨¡å‹éœ€è¦ä» **HuggingFace** ä¸‹è½½
- æ¨¡å‹ä¼šåœ¨é¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨ä¸‹è½½ï¼Œæˆ–å¯ä»¥æå‰ä¸‹è½½

### 2. LoRA å¾®è°ƒè¯¦ç»†æ­¥éª¤

ç”±äºä½ ä½¿ç”¨çš„æ˜¯ **A100 GPU**ï¼Œæ¨èä½¿ç”¨è¾ƒå¤§çš„æ¨¡å‹ï¼ˆå¦‚ Qwen2-7Bï¼‰ï¼ŒLoRA å¯ä»¥æ˜¾è‘—èŠ‚çœæ˜¾å­˜ã€‚

---

## âš ï¸ é‡è¦æç¤ºï¼šä»£ç éœ€è¦ä¿®æ”¹

**åŸå§‹ä»£ç ä¸æ”¯æŒ LoRA**ï¼Œéœ€è¦å…ˆä¿®æ”¹ `sft.py` å’Œ `rl.py` æ‰èƒ½ä½¿ç”¨ `--use_lora True` å‚æ•°ã€‚

æœ‰ä¸¤ç§æ–¹å¼ï¼š
1. **æ‰‹åŠ¨ä¿®æ”¹**ï¼šå‚è€ƒä¸‹é¢"æ­¥éª¤ 3ï¼šä¿®æ”¹ä»£ç ä»¥æ”¯æŒ LoRA"éƒ¨åˆ†
2. **æŸ¥çœ‹ç¤ºä¾‹**ï¼šå‚è€ƒ `LoRA_ä»£ç ä¿®æ”¹ç¤ºä¾‹.py` æ–‡ä»¶

ä¿®æ”¹å®Œæˆåï¼Œæ‰èƒ½è¿è¡Œ"å¿«é€Ÿå¼€å§‹"éƒ¨åˆ†çš„å‘½ä»¤ã€‚

---

## ğŸš€ å®Œæ•´æ­¥éª¤ï¼ˆA100 ç¯å¢ƒï¼‰

### æ­¥éª¤ 1ï¼šç¡®è®¤ç¯å¢ƒ

```bash
# æ£€æŸ¥ GPU
nvidia-smi

# æ£€æŸ¥ Python ç¯å¢ƒ
python --version  # åº”è¯¥æ˜¯ 3.10+

# æ£€æŸ¥ä¾èµ–
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import peft; print(f'PEFT: {peft.__version__}')"

# æ£€æŸ¥ huggingface-hubï¼ˆç”¨äºè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼‰
python -c "from huggingface_hub import __version__; print(f'huggingface-hub: {__version__}')"
```

**é‡è¦æç¤º**ï¼š
- âœ… **ä¸éœ€è¦æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹**ï¼Œä»£ç ä¼šåœ¨é¦–æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨ä» HuggingFace ä¸‹è½½
- âœ… æ¨¡å‹ä¼šä¸‹è½½åˆ° `~/.cache/huggingface/hub/` ç›®å½•
- âœ… å¦‚æœ HuggingFace è®¿é—®æ…¢ï¼Œå¯ä»¥è®¾ç½®é•œåƒï¼š`export HF_ENDPOINT=https://hf-mirror.com`

### æ­¥éª¤ 2ï¼šä¸‹è½½åŸºç¡€æ¨¡å‹ï¼ˆä¸éœ€è¦æ‰‹åŠ¨ä¸‹è½½ï¼ï¼‰

**âœ… é‡è¦ï¼šåŸºç¡€æ¨¡å‹ä¸éœ€è¦æ‰‹åŠ¨ä¸‹è½½ï¼Œä»£ç ä¼šè‡ªåŠ¨ä¸‹è½½ï¼**

#### æ¨èæ–¹å¼ï¼šä»£ç è‡ªåŠ¨ä¸‹è½½ï¼ˆæœ€ç®€å•ï¼‰

**ç›´æ¥è¿è¡Œè®­ç»ƒå‘½ä»¤å³å¯**ï¼Œä»£ç ä¼šåœ¨é¦–æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨ä» HuggingFace ä¸‹è½½æ¨¡å‹ï¼š

```bash
# ç›´æ¥è¿è¡Œï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° ~/.cache/huggingface/hub/
python sft.py --base_model Qwen/Qwen2-7B-Instruct ...
```

**æ¨¡å‹ä¸‹è½½ä½ç½®**ï¼š
- è‡ªåŠ¨ä¿å­˜åˆ°ï¼š`~/.cache/huggingface/hub/`
- ä¸‹è½½ä¸€æ¬¡åï¼Œåç»­è¿è¡Œä¼šç›´æ¥ä½¿ç”¨ç¼“å­˜
- ä¸éœ€è¦æ‰‹åŠ¨ç®¡ç†æ¨¡å‹æ–‡ä»¶

#### å¦‚æœ HuggingFace è®¿é—®æ…¢ï¼ˆè®¾ç½®é•œåƒï¼‰

```bash
# è®¾ç½®å›½å†…é•œåƒï¼ˆåœ¨è¿è¡Œè®­ç»ƒå‰ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# ç„¶åæ­£å¸¸è¿è¡Œè®­ç»ƒå‘½ä»¤
python sft.py --base_model Qwen/Qwen2-7B-Instruct ...
```

#### å¦‚æœéœ€è¦æ‰‹åŠ¨ä¸‹è½½ï¼ˆå¯é€‰ï¼‰

**æ–¹å¼ 1ï¼šä½¿ç”¨ Python ä»£ç ä¸‹è½½**

åˆ›å»º `download_model.py`ï¼š

```python
from huggingface_hub import snapshot_download

# ä¸‹è½½æ¨¡å‹åˆ°æŒ‡å®šç›®å½•
snapshot_download(
    repo_id="Qwen/Qwen2-7B-Instruct",
    local_dir="./models/Qwen2-7B-Instruct",
    local_dir_use_symlinks=False
)
print("æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
```

è¿è¡Œï¼š
```bash
python download_model.py
```

**æ–¹å¼ 2ï¼šä½¿ç”¨ Python æ¨¡å—æ–¹å¼è°ƒç”¨ huggingface-cli**

å¦‚æœ `huggingface-cli` å‘½ä»¤æ‰¾ä¸åˆ°ï¼Œå¯ä»¥ä½¿ç”¨ Python æ¨¡å—æ–¹å¼ï¼š

```bash
# ä½¿ç”¨ Python æ¨¡å—æ–¹å¼è°ƒç”¨
python -m huggingface_hub.cli.download \
    Qwen/Qwen2-7B-Instruct \
    --local-dir ./models/Qwen2-7B-Instruct
```

**æ³¨æ„**ï¼šå³ä½¿ `huggingface-cli` å‘½ä»¤ä¸å¯ç”¨ï¼Œä»£ç ä¹Ÿä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œ**ä¸éœ€è¦æ‰‹åŠ¨ä¸‹è½½**ï¼

**æ¨èæ¨¡å‹ï¼ˆA100 æ¨èï¼‰**ï¼š

| æ¨¡å‹ | æ¨èåº¦ | æ˜¾å­˜å ç”¨ (LoRA) | é€‚ç”¨åœºæ™¯ |
|------|--------|----------------|---------|
| `Qwen/Qwen2.5-7B-Instruct` | â­â­â­â­â­ | ~12GB | **A100 æœ€ä½³é€‰æ‹©**ï¼ˆæ€§èƒ½ä¸èµ„æºå¹³è¡¡ï¼‰ |
| `Qwen/Qwen2.5-3B-Instruct` | â­â­â­â­ | ~8GB | æ˜¾å­˜å—é™æˆ–éœ€è¦æ›´å¿«è®­ç»ƒ |
| `Qwen/Qwen2-7B-Instruct` | â­â­â­â­ | ~12GB | ç¨³å®šå¯é çš„é€‰æ‹© |
| `Qwen/Qwen2-3B-Instruct` | â­â­â­ | ~8GB | èµ„æºå—é™åœºæ™¯ |
| `Qwen/Qwen2.5-14B-Instruct` | â­â­â­â­â­ | ~24GB | å¤šå¡ A100ï¼Œè¿½æ±‚æœ€ä½³æ€§èƒ½ |

**é€‰æ‹©å»ºè®®**ï¼š
- âœ… **A100 å•å¡æ¨è**ï¼š`Qwen2.5-7B-Instruct`ï¼ˆæ€§èƒ½æœ€å¥½ï¼Œæ˜¾å­˜å……è¶³ï¼‰
- âœ… **å¦‚æœé€‰æ‹© 3B**ï¼š`Qwen2.5-3B-Instruct`ï¼ˆæ¯” Qwen2-3B æ€§èƒ½æ›´å¥½ï¼‰
- âœ… **Qwen2.5 ç³»åˆ—**æ˜¯ Qwen2 çš„å‡çº§ç‰ˆï¼Œ**å¼ºçƒˆæ¨èä½¿ç”¨æ–°ç‰ˆæœ¬**

### æ­¥éª¤ 3ï¼šä¿®æ”¹ä»£ç ä»¥æ”¯æŒ LoRA

#### 3.1 ä¿®æ”¹ `sft.py`

åœ¨ `sft.py` ä¸­æ·»åŠ  LoRA æ”¯æŒï¼š

```python
# åœ¨æ–‡ä»¶å¼€å¤´ï¼ˆç¬¬ 23 è¡Œé™„è¿‘ï¼‰æ·»åŠ å¯¼å…¥
from peft import LoraConfig, get_peft_model, TaskType
```

åœ¨ `train()` å‡½æ•°ä¸­æ·»åŠ å‚æ•°ï¼ˆç¬¬ 90 è¡Œé™„è¿‘ï¼‰ï¼š

```python
def train(
    # model/data params
    base_model: str = "",
    train_file: str="",
    eval_file: str="",
    output_dir: str = "",
    sample: int = -1,
    seed: int = 42,
    
    # training hyperparams
    batch_size: int = 128,
    micro_batch_size: int = 4,
    num_epochs: int = 10,
    learning_rate: float = 3e-4,
    cutoff_len: int = 512,
    # llm hyperparams
    group_by_length: bool = False,
    freeze_LLM: bool = False,
    # LoRA å‚æ•°ï¼ˆæ–°å¢ï¼‰
    use_lora: bool = False,
    lora_r: int = 16,
    lora_alpha: int = 32,
    lora_dropout: float = 0.05,
    lora_target_modules: str = "all",
    # wandb params
    wandb_project: str = "",
    wandb_run_name: str = "",
    resume_from_checkpoint: str = None,
    category: str="",
    train_from_scratch: bool = False,
    sid_index_path: str = "",
    item_meta_path: str = "",
):
```

åœ¨æ¨¡å‹åŠ è½½å’Œ SID tokens æ·»åŠ ä¹‹åï¼ˆç¬¬ 159 è¡Œä¹‹åï¼‰ï¼Œæ·»åŠ  LoRA é…ç½®ï¼š

```python
    if sid_index_path and os.path.exists(sid_index_path):
        print(f"Loading index from {sid_index_path}")
        token_extender = TokenExtender(
            data_path=os.path.dirname(sid_index_path),
            dataset=os.path.basename(sid_index_path).split('.')[0]
        )
        new_tokens = token_extender.get_new_tokens()
        if new_tokens:
            print(f"Adding {len(new_tokens)} new tokens to tokenizer")
            tokenizer.add_tokens(new_tokens)
            model.resize_token_embeddings(len(tokenizer))

    # ========== æ·»åŠ  LoRA é…ç½® ==========
    if use_lora:
        print("=" * 50)
        print("å¯ç”¨ LoRA å¾®è°ƒ")
        print("=" * 50)
        
        # ç¡®å®šç›®æ ‡æ¨¡å—
        if lora_target_modules == "all":
            # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç»“æ„
            if hasattr(model, 'model'):
                model_base = model.model
            else:
                model_base = model
            
            target_modules = []
            for name, module in model_base.named_modules():
                if any(x in name for x in ["q_proj", "k_proj", "v_proj", "o_proj", 
                                            "gate_proj", "up_proj", "down_proj"]):
                    target_modules.append(name.split('.')[-1])
            
            target_modules = list(set(target_modules))
            if not target_modules:
                target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        elif lora_target_modules == "qkv":
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        else:
            target_modules = [m.strip() for m in lora_target_modules.split(",")]
        
        print(f"LoRA ç›®æ ‡æ¨¡å—: {target_modules}")
        
        # åˆ›å»º LoRA é…ç½®
        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        # åº”ç”¨ LoRA
        model = get_peft_model(model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} "
              f"({100*trainable_params/total_params:.4f}%)")
        print("=" * 50)

    # Freeze LLM parameters if required
    if freeze_LLM:
        # ... ç°æœ‰ä»£ç ä¿æŒä¸å˜ ...
```

#### 3.2 ä¿®æ”¹ `rl.py`

åœ¨ `rl.py` ä¸­æ·»åŠ  LoRA æ”¯æŒï¼š

```python
# åœ¨æ–‡ä»¶å¼€å¤´ï¼ˆç¬¬ 8 è¡Œé™„è¿‘ï¼‰æ·»åŠ å¯¼å…¥
from peft import LoraConfig, TaskType
```

åœ¨ `train()` å‡½æ•°ä¸­æ·»åŠ å‚æ•°ï¼ˆç¬¬ 30 è¡Œé™„è¿‘ï¼‰ï¼š

```python
def train(
    # model/data params
    model_path: str = "",
    seed: int = 42,
    train_file: str = "",
    eval_file: str = "",
    info_file: str = "",
    category: str = "",
    
    # wandb params
    wandb_project: str = "",
    wandb_run_name: str = "",
    
    # training hyperparams
    output_dir: str = "",
    train_batch_size: int = 32,
    eval_batch_size: int = 32,
    gradient_accumulation_steps: int = 1,
    temperature: float = 1.0,
    add_gt: bool = False,
    eval_step: float = 0.199,
    num_generations: int = 16,
    num_train_epochs: int = 1,
    learning_rate: float = 1e-6,
    beta: float = 0.04,
    beam_search: bool = False,
    test_during_training: bool = True,
    dynamic_sampling: bool = False,
    mask_all_zero: bool = False,
    sync_ref_model: bool = False,
    test_beam: int = 20,
    reward_type: str = "rule",
    sample_train: bool = False,
    ada_path: str = "",
    cf_path: str = "",
    sid_index_path: str = "",
    item_meta_path: str = "",
    dapo: bool = False,
    gspo: bool = False,
    # LoRA å‚æ•°ï¼ˆæ–°å¢ï¼‰
    use_lora: bool = False,
    lora_r: int = 16,
    lora_alpha: int = 32,
    lora_dropout: float = 0.05,
    lora_target_modules: str = "all",
):
```

åœ¨åˆ›å»º `ReReTrainer` ä¹‹å‰ï¼ˆç¬¬ 288 è¡Œä¹‹å‰ï¼‰ï¼Œæ·»åŠ  LoRA é…ç½®ï¼š

```python
    # ========== é…ç½® LoRA ==========
    peft_config = None
    if use_lora:
        print("=" * 50)
        print("RL é˜¶æ®µå¯ç”¨ LoRA å¾®è°ƒ")
        print("=" * 50)
        
        # ç¡®å®šç›®æ ‡æ¨¡å—ï¼ˆéœ€è¦å…ˆåŠ è½½æ¨¡å‹æ¥æ£€æµ‹ç»“æ„ï¼‰
        if lora_target_modules == "all":
            from transformers import AutoModelForCausalLM
            temp_model = AutoModelForCausalLM.from_pretrained(
                model_path, 
                torch_dtype=torch.bfloat16,
                device_map="auto"
            )
            if hasattr(temp_model, 'model'):
                model_base = temp_model.model
            else:
                model_base = temp_model
            
            target_modules = []
            for name, module in model_base.named_modules():
                if any(x in name for x in ["q_proj", "k_proj", "v_proj", "o_proj",
                                            "gate_proj", "up_proj", "down_proj"]):
                    target_modules.append(name.split('.')[-1])
            target_modules = list(set(target_modules))
            if not target_modules:
                target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
            del temp_model
            torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜
        elif lora_target_modules == "qkv":
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        else:
            target_modules = [m.strip() for m in lora_target_modules.split(",")]
        
        print(f"LoRA ç›®æ ‡æ¨¡å—: {target_modules}")
        
        peft_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        print("=" * 50)

    training_args = GRPOConfig(output_dir=output_dir,
                                # ... ç°æœ‰å‚æ•° ...
                            )
    trainer = ReReTrainer(
        model=model_path,
        base_model=model_path,
        peft_config=peft_config,  # ä¼ å…¥ LoRA é…ç½®
        dapo=dapo,
        gspo=gspo,
        add_gt=add_gt,
        dynamic_sampling=dynamic_sampling,
        beam_search=beam_search,
        test_during_training=test_during_training,
        test_beam=test_beam,
        info_file=info_file,
        prompt2history=prompt2history,
        history2target=history2target,
        reward_funcs=reward_fun,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        args=training_args,
    )
```

### æ­¥éª¤ 4ï¼šå‡†å¤‡æ•°æ®

ç¡®è®¤æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼š

```bash
# æ£€æŸ¥æ•°æ®æ–‡ä»¶
ls -lh ./data/Amazon/index/Industrial_and_Scientific.*
ls -lh ./data/Amazon/train/Industrial_and_Scientific*
ls -lh ./data/Amazon/valid/Industrial_and_Scientific*
ls -lh ./data/Amazon/info/Industrial_and_Scientific*
```

åº”è¯¥çœ‹åˆ°ï¼š
- `Industrial_and_Scientific.item.json`
- `Industrial_and_Scientific.index.json`
- `Industrial_and_Scientific_5_2016-10-2018-11.csv` (train/valid/test)
- `Industrial_and_Scientific_5_2016-10-2018-11.txt` (info)

### æ­¥éª¤ 5ï¼šè¿è¡Œ LoRA å¾®è°ƒ

#### 5.1 SFT é˜¶æ®µï¼ˆä½¿ç”¨ LoRAï¼‰

**å•å¡è®­ç»ƒ**ï¼ˆA100 80GB æ¨èé…ç½®ï¼‰ï¼š

```bash
python sft.py \
    --base_model Qwen/Qwen2-7B-Instruct \
    --train_file ./data/Amazon/train/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --eval_file ./data/Amazon/valid/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --output_dir ./output/sft_lora \
    --use_lora True \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --lora_target_modules "all" \
    --batch_size 1024 \
    --micro_batch_size 16 \
    --num_epochs 10 \
    --learning_rate 3e-4 \
    --category Industrial_and_Scientific \
    --sid_index_path ./data/Amazon/index/Industrial_and_Scientific.index.json \
    --item_meta_path ./data/Amazon/index/Industrial_and_Scientific.item.json \
    --wandb_project minionerec_lora \
    --wandb_run_name sft_lora_qwen7b
```

**å¤šå¡è®­ç»ƒ**ï¼ˆå¦‚æœæœ‰å¤šä¸ª A100ï¼‰ï¼š

```bash
torchrun --nproc_per_node 8 \
    sft.py \
    --base_model Qwen/Qwen2-7B-Instruct \
    --train_file ./data/Amazon/train/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --eval_file ./data/Amazon/valid/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --output_dir ./output/sft_lora \
    --use_lora True \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --lora_target_modules "all" \
    --batch_size 1024 \
    --micro_batch_size 16 \
    --num_epochs 10 \
    --learning_rate 3e-4 \
    --category Industrial_and_Scientific \
    --sid_index_path ./data/Amazon/index/Industrial_and_Scientific.index.json \
    --item_meta_path ./data/Amazon/index/Industrial_and_Scientific.item.json \
    --wandb_project minionerec_lora \
    --wandb_run_name sft_lora_qwen7b
```

**é¢„æœŸè¾“å‡º**ï¼š
```
==================================================
å¯ç”¨ LoRA å¾®è°ƒ
==================================================
LoRA ç›®æ ‡æ¨¡å—: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
å¯è®­ç»ƒå‚æ•°: 13,107,200 / 7,000,000,000 (0.19%)
==================================================
```

#### 5.2 RL é˜¶æ®µï¼ˆä½¿ç”¨ LoRAï¼‰

**é‡è¦**ï¼šRL é˜¶æ®µå¿…é¡»ä½¿ç”¨ SFT é˜¶æ®µè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä¸” LoRA é…ç½®éœ€è¦ä¸€è‡´ã€‚

```bash
python rl.py \
    --model_path ./output/sft_lora/final_checkpoint \
    --train_file ./data/Amazon/train/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --eval_file ./data/Amazon/valid/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --output_dir ./output/rl_lora \
    --use_lora True \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --lora_target_modules "all" \
    --info_file ./data/Amazon/info/Industrial_and_Scientific_5_2016-10-2018-11.txt \
    --sid_index_path ./data/Amazon/index/Industrial_and_Scientific.index.json \
    --item_meta_path ./data/Amazon/index/Industrial_and_Scientific.item.json \
    --category Industrial_and_Scientific \
    --train_batch_size 32 \
    --eval_batch_size 32 \
    --num_generations 16 \
    --num_train_epochs 1 \
    --learning_rate 1e-6 \
    --beta 0.04 \
    --reward_type rule \
    --wandb_project minionerec_lora \
    --wandb_run_name rl_lora_qwen7b
```

### æ­¥éª¤ 6ï¼šéªŒè¯è®­ç»ƒç»“æœ

#### 6.1 æ£€æŸ¥è¾“å‡ºæ–‡ä»¶

```bash
# SFT è¾“å‡º
ls -lh ./output/sft_lora/
# åº”è¯¥çœ‹åˆ°ï¼š
# - adapter_config.jsonï¼ˆLoRA é…ç½®ï¼‰
# - adapter_model.binï¼ˆLoRA æƒé‡ï¼‰
# - final_checkpoint/ï¼ˆå®Œæ•´æ£€æŸ¥ç‚¹ï¼‰

# RL è¾“å‡º
ls -lh ./output/rl_lora/
```

#### 6.2 åŠ è½½ LoRA æ¨¡å‹è¿›è¡Œæ¨ç†

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2-7B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# åŠ è½½ LoRA é€‚é…å™¨
model = PeftModel.from_pretrained(
    base_model,
    "./output/sft_lora/final_checkpoint"
)

# åˆå¹¶ LoRA æƒé‡ï¼ˆå¯é€‰ï¼Œç”¨äºæ¨ç†åŠ é€Ÿï¼‰
# model = model.merge_and_unload()

tokenizer = AutoTokenizer.from_pretrained("./output/sft_lora/final_checkpoint")
```

---

## ğŸ“Š A100 æ¨èé…ç½®

### å•å¡ A100 (80GB)

| æ¨¡å‹ | LoRA æ˜¾å­˜å ç”¨ | æ¨è batch_size | æ¨è micro_batch_size | æ¨èåº¦ |
|------|-------------|----------------|---------------------|--------|
| **Qwen2.5-7B** | ~12GB | 1024 | 16 | â­â­â­â­â­ **æœ€ä½³** |
| **Qwen2.5-3B** | ~8GB | 1024 | 16-32 | â­â­â­â­ **å¿«é€Ÿè®­ç»ƒ** |
| Qwen2-7B | ~12GB | 1024 | 16 | â­â­â­â­ |
| Qwen2-3B | ~8GB | 1024 | 16-32 | â­â­â­ |
| Qwen2-14B | ~24GB | 512 | 8-16 | â­â­â­ |

### å¤šå¡ A100

ä½¿ç”¨ `torchrun` è¿›è¡Œå¤šå¡è®­ç»ƒï¼š

```bash
# 8 å¡è®­ç»ƒ
torchrun --nproc_per_node 8 sft.py ...
```

---

## âš™ï¸ LoRA å‚æ•°è°ƒä¼˜å»ºè®®

### æ¨èé…ç½®ï¼ˆA100ï¼‰

| åœºæ™¯ | r | alpha | dropout | target_modules |
|------|---|-------|---------|----------------|
| **å¿«é€Ÿå®éªŒ** | 8 | 16 | 0.05 | "qkv" |
| **å¹³è¡¡æ€§èƒ½** | 16 | 32 | 0.05 | "all" âœ… **æ¨è** |
| **æœ€ä½³æ€§èƒ½** | 32 | 64 | 0.1 | "all" |

### å‚æ•°è¯´æ˜

- **`r`**ï¼šLoRA rankï¼Œæ§åˆ¶é€‚é…å™¨å¤§å°
  - è¾ƒå°å€¼ï¼ˆ8-16ï¼‰ï¼šæ˜¾å­˜å ç”¨æ›´å°‘ï¼Œä½†å¯èƒ½æ¬ æ‹Ÿåˆ
  - è¾ƒå¤§å€¼ï¼ˆ32-64ï¼‰ï¼šæ€§èƒ½æ›´å¥½ï¼Œä½†æ˜¾å­˜å ç”¨å¢åŠ 
  - **æ¨è**ï¼š16ï¼ˆå¹³è¡¡æ€§èƒ½å’Œèµ„æºï¼‰

- **`lora_alpha`**ï¼šLoRA çš„ç¼©æ”¾å› å­
  - é€šå¸¸è®¾ç½®ä¸º `r` çš„ 2 å€
  - **æ¨è**ï¼š`alpha = 2 * r`ï¼ˆå³ 32ï¼‰

- **`lora_dropout`**ï¼šDropout ç‡
  - **æ¨è**ï¼š0.05-0.1

- **`lora_target_modules`**ï¼šç›®æ ‡æ¨¡å—
  - `"all"`ï¼šè‡ªåŠ¨æ£€æµ‹æ‰€æœ‰æ³¨æ„åŠ›å±‚å’Œ MLP å±‚ï¼ˆæ¨èï¼‰
  - `"qkv"`ï¼šåªé’ˆå¯¹æ³¨æ„åŠ›å±‚ï¼ˆq_proj, k_proj, v_proj, o_projï¼‰
  - **æ¨è**ï¼š`"all"`ï¼ˆæ€§èƒ½æœ€å¥½ï¼‰

---

## ğŸ” å¸¸è§é—®é¢˜

### 1. huggingface-cli å‘½ä»¤æœªæ‰¾åˆ°

**é—®é¢˜**ï¼š`huggingface-cli: command not found`

**è§£å†³æ–¹æ¡ˆ**ï¼š

**âœ… æœ€ç®€å•çš„æ–¹æ³•ï¼šä¸éœ€è¦ä½¿ç”¨ huggingface-cliï¼**

ä»£ç ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œç›´æ¥è¿è¡Œè®­ç»ƒå‘½ä»¤å³å¯ï¼š

```bash
python sft.py --base_model Qwen/Qwen2-7B-Instruct ...
```

**å¦‚æœéœ€è¦æ‰‹åŠ¨ä¸‹è½½ï¼Œä½¿ç”¨ Python æ¨¡å—æ–¹å¼**ï¼š

```bash
# æ–¹å¼ 1ï¼šä½¿ç”¨ Python æ¨¡å—è°ƒç”¨ï¼ˆæ¨èï¼‰
python -m huggingface_hub.cli.download \
    Qwen/Qwen2-7B-Instruct \
    --local-dir ./models/Qwen2-7B-Instruct

# æ–¹å¼ 2ï¼šä½¿ç”¨ Python ä»£ç 
python -c "from huggingface_hub import snapshot_download; snapshot_download('Qwen/Qwen2-7B-Instruct', local_dir='./models/Qwen2-7B-Instruct')"
```

**å¦‚æœ HuggingFace è®¿é—®æ…¢**ï¼š

```bash
# è®¾ç½®é•œåƒç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# ç„¶åæ­£å¸¸è¿è¡Œè®­ç»ƒå‘½ä»¤
python sft.py --base_model Qwen/Qwen2-7B-Instruct ...
```

**é‡è¦**ï¼šå³ä½¿ `huggingface-cli` å‘½ä»¤ä¸å¯ç”¨ï¼Œä»£ç ä¹Ÿä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œ**ä¸éœ€è¦æ‰‹åŠ¨ä¸‹è½½**ï¼

### 2. æ˜¾å­˜ä¸è¶³ï¼ˆOOMï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡å° `batch_size` æˆ– `micro_batch_size`
- ä½¿ç”¨æ›´å°çš„ `lora_r`ï¼ˆå¦‚ 8ï¼‰
- ä½¿ç”¨ `lora_target_modules="qkv"`ï¼ˆåªè®­ç»ƒæ³¨æ„åŠ›å±‚ï¼‰

### 3. LoRA è®­ç»ƒåå¦‚ä½•åŠ è½½

```python
# æ–¹å¼ 1ï¼šåŠ è½½ LoRA é€‚é…å™¨ï¼ˆæ¨èï¼‰
from peft import PeftModel
model = PeftModel.from_pretrained(base_model, "./output/sft_lora/final_checkpoint")

# æ–¹å¼ 2ï¼šåˆå¹¶æƒé‡ï¼ˆç”¨äºæ¨ç†åŠ é€Ÿï¼‰
model = model.merge_and_unload()
model.save_pretrained("./output/sft_lora_merged")
```

### 4. SFT å’Œ RL é˜¶æ®µ LoRA é…ç½®ä¸ä¸€è‡´

**é‡è¦**ï¼šSFT å’Œ RL é˜¶æ®µçš„ LoRA é…ç½®ï¼ˆ`r`, `alpha`, `target_modules`ï¼‰**å¿…é¡»ä¸€è‡´**ï¼Œå¦åˆ™æ— æ³•åŠ è½½ã€‚

---

## ğŸ“ å®Œæ•´ç¤ºä¾‹è„šæœ¬

åˆ›å»º `run_lora_training.sh`ï¼š

```bash
#!/bin/bash

# è®¾ç½®ç¯å¢ƒå˜é‡
export NCCL_IB_DISABLE=1
export CUDA_VISIBLE_DEVICES=0  # å•å¡è®­ç»ƒï¼Œå¤šå¡è¯·ä½¿ç”¨ torchrun

# æ•°æ®é›†é…ç½®
CATEGORY="Industrial_and_Scientific"
TRAIN_FILE="./data/Amazon/train/${CATEGORY}_5_2016-10-2018-11.csv"
EVAL_FILE="./data/Amazon/valid/${CATEGORY}_5_2016-10-2018-11.csv"
INFO_FILE="./data/Amazon/info/${CATEGORY}_5_2016-10-2018-11.txt"
SID_INDEX="./data/Amazon/index/${CATEGORY}.index.json"
ITEM_META="./data/Amazon/index/${CATEGORY}.item.json"

# æ¨¡å‹é…ç½®
BASE_MODEL="Qwen/Qwen2-7B-Instruct"

# LoRA é…ç½®
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.05
LORA_TARGET="all"

# è®­ç»ƒé…ç½®
BATCH_SIZE=1024
MICRO_BATCH=16
NUM_EPOCHS=10
LEARNING_RATE=3e-4

echo "=========================================="
echo "å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ"
echo "=========================================="
echo "æ•°æ®é›†: ${CATEGORY}"
echo "åŸºç¡€æ¨¡å‹: ${BASE_MODEL}"
echo "LoRA é…ç½®: r=${LORA_R}, alpha=${LORA_ALPHA}"
echo "=========================================="

# SFT é˜¶æ®µ
echo "æ­¥éª¤ 1: SFT è®­ç»ƒï¼ˆLoRAï¼‰"
python sft.py \
    --base_model ${BASE_MODEL} \
    --train_file ${TRAIN_FILE} \
    --eval_file ${EVAL_FILE} \
    --output_dir ./output/sft_lora_${CATEGORY} \
    --use_lora True \
    --lora_r ${LORA_R} \
    --lora_alpha ${LORA_ALPHA} \
    --lora_dropout ${LORA_DROPOUT} \
    --lora_target_modules ${LORA_TARGET} \
    --batch_size ${BATCH_SIZE} \
    --micro_batch_size ${MICRO_BATCH} \
    --num_epochs ${NUM_EPOCHS} \
    --learning_rate ${LEARNING_RATE} \
    --category ${CATEGORY} \
    --sid_index_path ${SID_INDEX} \
    --item_meta_path ${ITEM_META} \
    --wandb_project minionerec_lora \
    --wandb_run_name sft_lora_${CATEGORY}

echo "SFT è®­ç»ƒå®Œæˆï¼"

# RL é˜¶æ®µ
echo "æ­¥éª¤ 2: RL è®­ç»ƒï¼ˆLoRAï¼‰"
python rl.py \
    --model_path ./output/sft_lora_${CATEGORY}/final_checkpoint \
    --train_file ${TRAIN_FILE} \
    --eval_file ${EVAL_FILE} \
    --output_dir ./output/rl_lora_${CATEGORY} \
    --use_lora True \
    --lora_r ${LORA_R} \
    --lora_alpha ${LORA_ALPHA} \
    --lora_dropout ${LORA_DROPOUT} \
    --lora_target_modules ${LORA_TARGET} \
    --info_file ${INFO_FILE} \
    --sid_index_path ${SID_INDEX} \
    --item_meta_path ${ITEM_META} \
    --category ${CATEGORY} \
    --train_batch_size 32 \
    --eval_batch_size 32 \
    --num_generations 16 \
    --num_train_epochs 1 \
    --learning_rate 1e-6 \
    --beta 0.04 \
    --reward_type rule \
    --wandb_project minionerec_lora \
    --wandb_run_name rl_lora_${CATEGORY}

echo "RL è®­ç»ƒå®Œæˆï¼"
echo "=========================================="
echo "è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨ï¼š"
echo "SFT: ./output/sft_lora_${CATEGORY}/final_checkpoint"
echo "RL:  ./output/rl_lora_${CATEGORY}/final_checkpoint"
echo "=========================================="
```

ä½¿ç”¨ï¼š

```bash
chmod +x run_lora_training.sh
./run_lora_training.sh
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”ï¼ˆA100ï¼‰

| å¾®è°ƒæ–¹å¼ | æ˜¾å­˜å ç”¨ | è®­ç»ƒé€Ÿåº¦ | æ¨¡å‹å¤§å° |
|---------|---------|---------|---------|
| **å…¨å‚æ•°å¾®è°ƒ** | ~28GB | åŸºå‡† | 7B å‚æ•° |
| **LoRA (r=16)** | ~12GB | **1.5-2.0x æ›´å¿«** | ~13M å‚æ•° |

**ä¼˜åŠ¿**ï¼š
- âœ… æ˜¾å­˜å ç”¨é™ä½ **57%**
- âœ… è®­ç»ƒé€Ÿåº¦æå‡ **50-100%**
- âœ… æ¨¡å‹æ–‡ä»¶æ›´å°ï¼ˆåªéœ€ä¿å­˜ LoRA æƒé‡ï¼‰
- âœ… æ€§èƒ½å·®è·å¾ˆå°ï¼ˆé€šå¸¸ < 3%ï¼‰

---

## âœ… æ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹è®­ç»ƒå‰ï¼Œç¡®è®¤ï¼š

- [ ] ç¯å¢ƒå·²é…ç½®ï¼ˆ`pip install -r requirements.txt`ï¼‰
- [ ] PEFT å·²å®‰è£…ï¼ˆ`pip install peft`ï¼‰
- [ ] GPU å¯ç”¨ï¼ˆ`nvidia-smi` æ˜¾ç¤º A100ï¼‰
- [ ] æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼ˆ`.item.json`, `.index.json`, CSV æ–‡ä»¶ï¼‰
- [ ] ä»£ç å·²ä¿®æ”¹ï¼ˆ`sft.py` å’Œ `rl.py` æ·»åŠ äº† LoRA æ”¯æŒï¼‰
- [ ] åŸºç¡€æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼ˆHuggingFace æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„ï¼‰

---

## ğŸ¯ æ€»ç»“

1. **åŸºç¡€æ¨¡å‹**ï¼šé¡¹ç›®ä¸­æ²¡æœ‰ï¼Œéœ€è¦ä» HuggingFace ä¸‹è½½ï¼ˆä»£ç ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
2. **LoRA å¾®è°ƒæ­¥éª¤**ï¼š
   - ä¿®æ”¹ `sft.py` æ·»åŠ  LoRA æ”¯æŒ
   - ä¿®æ”¹ `rl.py` æ·»åŠ  LoRA æ”¯æŒ
   - è¿è¡Œ SFT è®­ç»ƒï¼ˆä½¿ç”¨ `--use_lora True`ï¼‰
   - è¿è¡Œ RL è®­ç»ƒï¼ˆä½¿ç”¨ `--use_lora True`ï¼Œä¸”é…ç½®ä¸ SFT ä¸€è‡´ï¼‰
3. **A100 ä¼˜åŠ¿**ï¼šå¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆ7B+ï¼‰å’Œæ›´å¤§çš„ batch size

**æ¨èé…ç½®ï¼ˆA100ï¼‰**ï¼š
- æ¨¡å‹ï¼š`Qwen/Qwen2.5-7B-Instruct`ï¼ˆæœ€ä½³æ€§èƒ½ï¼‰æˆ– `Qwen/Qwen2.5-3B-Instruct`ï¼ˆæ›´å¿«è®­ç»ƒï¼‰
- LoRA: `r=16, alpha=32, dropout=0.05, target_modules="all"`
- Batch size: `1024` (micro_batch_size: `16` for 7B, `32` for 3B)

---

## ğŸš€ æ¨¡å‹å·²ä¸‹è½½åçš„ä¸‹ä¸€æ­¥æ“ä½œ

### æ­¥éª¤ 1ï¼šç¡®è®¤æ¨¡å‹ä½ç½®

**æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨é»˜è®¤ä½ç½®**ï¼š

```bash
# æ£€æŸ¥ HuggingFace ç¼“å­˜ç›®å½•
ls -lh ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/

# æˆ–è€…å¦‚æœä¸‹è½½åˆ°äº†è‡ªå®šä¹‰ç›®å½•ï¼Œç¡®è®¤è·¯å¾„
# ä¾‹å¦‚ï¼š./models/Qwen2.5-3B-Instruct
```

**æ¨¡å‹è·¯å¾„è¯´æ˜**ï¼š
- **å¦‚æœæ¨¡å‹åœ¨é»˜è®¤ä½ç½®**ï¼ˆ`~/.cache/huggingface/hub/`ï¼‰ï¼š
  - ç›´æ¥ä½¿ç”¨æ¨¡å‹ IDï¼š`Qwen/Qwen2.5-3B-Instruct`
  - ä»£ç ä¼šè‡ªåŠ¨æ‰¾åˆ°æ¨¡å‹
- **å¦‚æœæ¨¡å‹åœ¨è‡ªå®šä¹‰ç›®å½•**ï¼ˆå¦‚ `./models/Qwen2.5-3B-Instruct`ï¼‰ï¼š
  - ä½¿ç”¨å®Œæ•´è·¯å¾„ï¼š`./models/Qwen2.5-3B-Instruct`
  - æˆ–ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼š`/path/to/models/Qwen2.5-3B-Instruct`

### æ­¥éª¤ 2ï¼šå¼€å§‹ SFT è®­ç»ƒï¼ˆLoRAï¼‰

**æ–¹å¼ 1ï¼šä½¿ç”¨è„šæœ¬ï¼ˆæ¨èï¼‰**

```bash
# ç¼–è¾‘ run_lora_sft.shï¼Œç¡®è®¤æ¨¡å‹è·¯å¾„
# ç„¶åè¿è¡Œ
bash run_lora_sft.sh
```

**æ–¹å¼ 2ï¼šç›´æ¥è¿è¡Œå‘½ä»¤**

**å¦‚æœæ¨¡å‹åœ¨é»˜è®¤ä½ç½®**ï¼š

```bash
python sft.py \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --train_file ./data/Amazon/train/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --eval_file ./data/Amazon/valid/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --output_dir ./output/sft_lora_qwen25_3b \
    --use_lora True \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --lora_target_modules "all" \
    --batch_size 1024 \
    --micro_batch_size 32 \
    --num_epochs 10 \
    --learning_rate 3e-4 \
    --category Industrial_and_Scientific \
    --sid_index_path ./data/Amazon/index/Industrial_and_Scientific.index.json \
    --item_meta_path ./data/Amazon/index/Industrial_and_Scientific.item.json \
    --wandb_project minionerec_lora \
    --wandb_run_name sft_lora_qwen25_3b
```

**å¦‚æœæ¨¡å‹åœ¨è‡ªå®šä¹‰ç›®å½•**ï¼š

```bash
python sft.py \
    --base_model ./models/Qwen2.5-3B-Instruct \
    --train_file ./data/Amazon/train/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --eval_file ./data/Amazon/valid/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --output_dir ./output/sft_lora_qwen25_3b \
    --use_lora True \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --lora_target_modules "all" \
    --batch_size 1024 \
    --micro_batch_size 32 \
    --num_epochs 10 \
    --learning_rate 3e-4 \
    --category Industrial_and_Scientific \
    --sid_index_path ./data/Amazon/index/Industrial_and_Scientific.index.json \
    --item_meta_path ./data/Amazon/index/Industrial_and_Scientific.item.json \
    --wandb_project minionerec_lora \
    --wandb_run_name sft_lora_qwen25_3b
```

### æ­¥éª¤ 3ï¼šç›‘æ§è®­ç»ƒ

**æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ**ï¼š

```bash
# åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œ
watch -n 1 nvidia-smi
```

**é¢„æœŸè¾“å‡º**ï¼š

è®­ç»ƒå¼€å§‹åï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š

```
==================================================
å¯ç”¨ LoRA å¾®è°ƒ
==================================================
LoRA ç›®æ ‡æ¨¡å—: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
å¯è®­ç»ƒå‚æ•°: 13,107,200 / 3,000,000,000 (0.44%)
==================================================
Loading index from ./data/Amazon/index/Industrial_and_Scientific.index.json
Adding 765 new tokens to tokenizer
LOAD DATA FINISHED
...
```

### æ­¥éª¤ 4ï¼šè®­ç»ƒæ—¶é—´ä¼°ç®—

å¯¹äº `Qwen2.5-3B-Instruct` + LoRAï¼š
- **å•å¡ A100**ï¼šçº¦ **2-4 å°æ—¶**ï¼ˆå–å†³äºæ•°æ®é‡ï¼‰
- **è®­ç»ƒé€Ÿåº¦**ï¼šæ¯” 7B æ¨¡å‹å¿«çº¦ 30-50%

### æ­¥éª¤ 5ï¼šè®­ç»ƒå®Œæˆåçš„ RL è®­ç»ƒ

SFT è®­ç»ƒå®Œæˆåï¼Œè¿è¡Œ RL è®­ç»ƒï¼š

```bash
python rl.py \
    --model_path ./output/sft_lora_qwen25_3b/final_checkpoint \
    --train_file ./data/Amazon/train/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --eval_file ./data/Amazon/valid/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --output_dir ./output/rl_lora_qwen25_3b \
    --use_lora True \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --lora_target_modules "all" \
    --info_file ./data/Amazon/info/Industrial_and_Scientific_5_2016-10-2018-11.txt \
    --sid_index_path ./data/Amazon/index/Industrial_and_Scientific.index.json \
    --item_meta_path ./data/Amazon/index/Industrial_and_Scientific.item.json \
    --category Industrial_and_Scientific \
    --train_batch_size 32 \
    --eval_batch_size 32 \
    --num_generations 16 \
    --num_train_epochs 1 \
    --learning_rate 1e-6 \
    --beta 0.04 \
    --reward_type rule \
    --wandb_project minionerec_lora \
    --wandb_run_name rl_lora_qwen25_3b
```

**é‡è¦**ï¼šRL é˜¶æ®µçš„ LoRA é…ç½®ï¼ˆ`r`, `alpha`, `target_modules`ï¼‰**å¿…é¡»ä¸ SFT é˜¶æ®µä¸€è‡´**ï¼

---

## ğŸ“Š æ­¥éª¤ 6ï¼šè¯„ä¼°æ¨¡å‹æ•ˆæœ

### âš ï¸ é‡è¦ï¼šLoRA æ¨¡å‹è¯„ä¼°æ–¹æ³•

ç”±äº LoRA æ¨¡å‹åŒ…å«æ‰©å±•çš„è¯è¡¨ï¼ˆæ·»åŠ äº†æ–°çš„ SID tokensï¼‰ï¼Œ**ä¸èƒ½ç›´æ¥ä½¿ç”¨åŸå§‹çš„ `evaluate.py`**ã€‚éœ€è¦ä½¿ç”¨ä¸“é—¨çš„ `evaluate_lora.py` è„šæœ¬ã€‚

### 6.1 è¯„ä¼° SFT æ¨¡å‹

**æ–¹å¼ Aï¼šä½¿ç”¨è„šæœ¬ï¼ˆæ¨èï¼‰**

```bash
bash evaluate_sft_lora.sh
```

**æ–¹å¼ Bï¼šç›´æ¥è¿è¡Œ Python**

```bash
python evaluate_lora.py \
    --base_model ./models/qwen3b \
    --lora_model ./output/sft_lora_qwen25_3b/final_checkpoint \
    --test_data_path ./data/Amazon/test/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --info_file ./data/Amazon/info/Industrial_and_Scientific_5_2016-10-2018-11.txt \
    --category Industrial_and_Scientific \
    --num_beams 50 \
    --K 10
```

**å‚æ•°è¯´æ˜**ï¼š
- `--base_model`ï¼š**åŸå§‹åŸºç¡€æ¨¡å‹è·¯å¾„**ï¼ˆå¦‚ `./models/qwen3b` æˆ– `Qwen/Qwen2.5-3B-Instruct`ï¼‰
- `--lora_model`ï¼š**LoRA æ¨¡å‹è·¯å¾„**ï¼ˆå¦‚ `./output/sft_lora_qwen25_3b/final_checkpoint`ï¼‰
- `--test_data_path`ï¼šæµ‹è¯•æ•°æ®è·¯å¾„
- `--info_file`ï¼šå•†å“ä¿¡æ¯æ–‡ä»¶
- `--category`ï¼šå•†å“ç±»åˆ«
- `--num_beams`ï¼šæŸæœç´¢å¤§å°ï¼ˆæ¨è 50ï¼‰
- `--K`ï¼šTop-K è¯„ä¼°ï¼ˆæ¨è 10ï¼‰

### 6.2 è¯„ä¼° RL æ¨¡å‹ï¼ˆRL è®­ç»ƒå®Œæˆåï¼‰

**æ–¹å¼ Aï¼šä½¿ç”¨è„šæœ¬ï¼ˆæ¨èï¼‰**

```bash
bash evaluate_rl_lora.sh
```

**æ–¹å¼ Bï¼šç›´æ¥è¿è¡Œ Python**

```bash
python evaluate_lora.py \
    --base_model ./models/qwen3b \
    --lora_model ./output/rl_lora_qwen25_3b/final_checkpoint \
    --test_data_path ./data/Amazon/test/Industrial_and_Scientific_5_2016-10-2018-11.csv \
    --info_file ./data/Amazon/info/Industrial_and_Scientific_5_2016-10-2018-11.txt \
    --category Industrial_and_Scientific \
    --num_beams 50 \
    --K 10
```

### 6.3 è¯„ä¼°æŒ‡æ ‡è¯´æ˜

è¯„ä¼°å®Œæˆåï¼Œä¼šè¾“å‡ºä»¥ä¸‹æŒ‡æ ‡ï¼š

```
==================================================
è¯„ä¼°ç»“æœ
==================================================
æµ‹è¯•æ ·æœ¬æ•°: 1000
HR@10: 0.3542
NDCG@10: 0.2156
MRR: 0.1834
==================================================
```

**æŒ‡æ ‡å«ä¹‰**ï¼š
- **HR@10 (Hit Rate@10)**ï¼šTop-10 æ¨èä¸­å‘½ä¸­ç›®æ ‡å•†å“çš„æ¯”ä¾‹
  - å€¼è¶Šé«˜è¶Šå¥½ï¼ŒèŒƒå›´ [0, 1]
  - ä¾‹å¦‚ 0.3542 è¡¨ç¤º 35.42% çš„æµ‹è¯•æ ·æœ¬åœ¨ Top-10 ä¸­æ‰¾åˆ°äº†ç›®æ ‡å•†å“

- **NDCG@10 (Normalized Discounted Cumulative Gain@10)**ï¼šè€ƒè™‘æ’åä½ç½®çš„æ¨èè´¨é‡
  - å€¼è¶Šé«˜è¶Šå¥½ï¼ŒèŒƒå›´ [0, 1]
  - æ’åè¶Šé å‰çš„å‘½ä¸­æƒé‡è¶Šé«˜

- **MRR (Mean Reciprocal Rank)**ï¼šç›®æ ‡å•†å“æ’åçš„å€’æ•°çš„å¹³å‡å€¼
  - å€¼è¶Šé«˜è¶Šå¥½ï¼ŒèŒƒå›´ [0, 1]
  - ä¾‹å¦‚ç›®æ ‡å•†å“æ’ç¬¬ 3ï¼Œåˆ™ RR = 1/3

### 6.4 å¯¹æ¯” SFT å’Œ RL æ¨¡å‹

å»ºè®®åŒæ—¶è¯„ä¼° SFT å’Œ RL æ¨¡å‹ï¼Œå¯¹æ¯”æ•ˆæœï¼š

```bash
# 1. è¯„ä¼° SFT æ¨¡å‹
bash evaluate_sft_lora.sh

# 2. è¿è¡Œ RL è®­ç»ƒ
bash rl_lora.sh

# 3. è¯„ä¼° RL æ¨¡å‹
bash evaluate_rl_lora.sh
```

**é¢„æœŸç»“æœ**ï¼š
- RL æ¨¡å‹çš„ HR@10 å’Œ NDCG@10 é€šå¸¸ä¼šæ¯” SFT æ¨¡å‹æå‡ **5-15%**
- è®­ç»ƒæ—¶é—´ï¼šSFT (2-4h) + RL (1-2h) = æ€»å…± 3-6h

### 6.5 è¯„ä¼°ç»“æœä¿å­˜

è¯„ä¼°ç»“æœä¼šè‡ªåŠ¨ä¿å­˜åˆ°ï¼š
- SFT æ¨¡å‹ï¼š`./output/sft_lora_qwen25_3b/eval_results.json`
- RL æ¨¡å‹ï¼š`./output/rl_lora_qwen25_3b/eval_results.json`

å¯ä»¥æŸ¥çœ‹è¯¦ç»†ç»“æœï¼š
```bash
cat ./output/sft_lora_qwen25_3b/eval_results.json
cat ./output/rl_lora_qwen25_3b/eval_results.json
```

---

## ğŸ“ˆ æ­¥éª¤ 7ï¼šå¦‚ä½•åˆ¤æ–­è®­ç»ƒæ•ˆæœå¥½å

### 7.1 æŸ¥çœ‹è¯„ä¼°ç»“æœæ–‡ä»¶

è¯„ä¼°å®Œæˆåï¼Œç»“æœä¿å­˜åœ¨ JSON æ–‡ä»¶ä¸­ã€‚æ¯ä¸ªæµ‹è¯•æ ·æœ¬åŒ…å«ï¼š

```bash
cat ./output/sft_lora_qwen25_3b/eval_results.json | head -50
```

**JSON æ ¼å¼ç¤ºä¾‹**ï¼š
```json
[
  {
    "input": "The user has interacted with items <a_123><b_45><c_67>, <a_234><b_56><c_78> in chronological order. Can you predict the next possible item?",
    "output": "<a_145><b_67><c_89>",
    "predict": [
      "<a_145><b_67><c_89>",     // Top-1 é¢„æµ‹ï¼ˆæ­£ç¡®ï¼ï¼‰
      "<a_150><b_70><c_90>",     // Top-2 é¢„æµ‹
      "<a_140><b_65><c_85>",     // Top-3 é¢„æµ‹
      ...
    ]
  },
  ...
]
```

### 7.2 è®¡ç®—è¯„ä¼°æŒ‡æ ‡

ä½¿ç”¨æä¾›çš„åˆ†æè„šæœ¬è®¡ç®—æŒ‡æ ‡ï¼š

```bash
python analyze_results.py \
    --result_file ./output/sft_lora_qwen25_3b/eval_results.json \
    --K 10
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
==================================================
è¯„ä¼°æŒ‡æ ‡åˆ†æ
==================================================
æµ‹è¯•æ ·æœ¬æ•°: 1000
Top-K: 10

HR@1:  0.1234  (12.34%)  â† Top-1 å‘½ä¸­ç‡
HR@5:  0.2856  (28.56%)  â† Top-5 å‘½ä¸­ç‡
HR@10: 0.3542  (35.42%)  â† Top-10 å‘½ä¸­ç‡
HR@20: 0.4123  (41.23%)  â† Top-20 å‘½ä¸­ç‡

NDCG@5:  0.1823
NDCG@10: 0.2156
NDCG@20: 0.2489

MRR: 0.1834

å¹³å‡é¢„æµ‹æ’å: 8.45
==================================================
```

### 7.3 è¯„ä¼°æŒ‡æ ‡è¯¦è§£

#### ğŸ“Š HR@K (Hit Rate @ K) - å‘½ä¸­ç‡

**å«ä¹‰**ï¼šåœ¨ Top-K æ¨èä¸­ï¼Œç›®æ ‡å•†å“å‡ºç°çš„æ¯”ä¾‹

**è®¡ç®—å…¬å¼**ï¼š
```
HR@K = (Top-K ä¸­åŒ…å«ç›®æ ‡å•†å“çš„æ ·æœ¬æ•°) / (æ€»æ ·æœ¬æ•°)
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- **HR@10 â‰¥ 0.30 (30%)**ï¼šâœ… **è‰¯å¥½**
- **HR@10 = 0.20-0.30 (20-30%)**ï¼šâš ï¸ **ä¸€èˆ¬**
- **HR@10 < 0.20 (20%)**ï¼šâŒ **è¾ƒå·®**

**ç¤ºä¾‹**ï¼š
- HR@10 = 0.35 è¡¨ç¤ºï¼š35% çš„æµ‹è¯•æ ·æœ¬ï¼Œç›®æ ‡å•†å“åœ¨ Top-10 æ¨èä¸­
- HR@1 = 0.12 è¡¨ç¤ºï¼š12% çš„æµ‹è¯•æ ·æœ¬ï¼Œç›®æ ‡å•†å“æ˜¯ç¬¬ä¸€ä¸ªæ¨è

#### ğŸ“Š NDCG@K (Normalized Discounted Cumulative Gain @ K) - æ’åè´¨é‡

**å«ä¹‰**ï¼šè€ƒè™‘æ’åä½ç½®çš„æ¨èè´¨é‡ï¼Œæ’åè¶Šé å‰æƒé‡è¶Šé«˜

**è®¡ç®—å…¬å¼**ï¼š
```
DCG@K = Î£ (rel_i / log2(i+1))  # rel_i = 1 å¦‚æœå‘½ä¸­ï¼Œå¦åˆ™ 0
NDCG@K = DCG@K / IDCG@K        # å½’ä¸€åŒ–
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- **NDCG@10 â‰¥ 0.20**ï¼šâœ… **è‰¯å¥½**
- **NDCG@10 = 0.15-0.20**ï¼šâš ï¸ **ä¸€èˆ¬**
- **NDCG@10 < 0.15**ï¼šâŒ **è¾ƒå·®**

**ç¤ºä¾‹**ï¼š
- ç›®æ ‡å•†å“æ’ç¬¬ 1ï¼šNDCG è´¡çŒ® = 1.0
- ç›®æ ‡å•†å“æ’ç¬¬ 2ï¼šNDCG è´¡çŒ® = 0.63
- ç›®æ ‡å•†å“æ’ç¬¬ 5ï¼šNDCG è´¡çŒ® = 0.43
- ç›®æ ‡å•†å“æ’ç¬¬ 10ï¼šNDCG è´¡çŒ® = 0.30

#### ğŸ“Š MRR (Mean Reciprocal Rank) - å¹³å‡å€’æ•°æ’å

**å«ä¹‰**ï¼šç›®æ ‡å•†å“æ’åçš„å€’æ•°çš„å¹³å‡å€¼

**è®¡ç®—å…¬å¼**ï¼š
```
RR = 1 / rank  # ç›®æ ‡å•†å“çš„æ’å
MRR = å¹³å‡æ‰€æœ‰æ ·æœ¬çš„ RR
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- **MRR â‰¥ 0.18**ï¼šâœ… **è‰¯å¥½**
- **MRR = 0.12-0.18**ï¼šâš ï¸ **ä¸€èˆ¬**
- **MRR < 0.12**ï¼šâŒ **è¾ƒå·®**

**ç¤ºä¾‹**ï¼š
- ç›®æ ‡å•†å“æ’ç¬¬ 1ï¼šRR = 1.0
- ç›®æ ‡å•†å“æ’ç¬¬ 2ï¼šRR = 0.5
- ç›®æ ‡å•†å“æ’ç¬¬ 5ï¼šRR = 0.2
- ç›®æ ‡å•†å“æ’ç¬¬ 10ï¼šRR = 0.1

### 7.4 ä¸åŒæ•°æ®é›†çš„åŸºå‡†æ€§èƒ½

#### Amazon Industrial_and_Scientific (æœ¬é¡¹ç›®é»˜è®¤æ•°æ®é›†)

| æ¨¡å‹ | HR@10 | NDCG@10 | MRR |
|------|-------|---------|-----|
| **ä¼˜ç§€** | > 0.35 | > 0.22 | > 0.19 |
| **è‰¯å¥½** | 0.30-0.35 | 0.18-0.22 | 0.15-0.19 |
| **ä¸€èˆ¬** | 0.25-0.30 | 0.15-0.18 | 0.12-0.15 |
| **è¾ƒå·®** | < 0.25 | < 0.15 | < 0.12 |

#### Amazon Office_Products

| æ¨¡å‹ | HR@10 | NDCG@10 | MRR |
|------|-------|---------|-----|
| **ä¼˜ç§€** | > 0.40 | > 0.25 | > 0.22 |
| **è‰¯å¥½** | 0.35-0.40 | 0.20-0.25 | 0.18-0.22 |
| **ä¸€èˆ¬** | 0.30-0.35 | 0.16-0.20 | 0.14-0.18 |
| **è¾ƒå·®** | < 0.30 | < 0.16 | < 0.14 |

**æ³¨æ„**ï¼šä¸åŒæ•°æ®é›†çš„éš¾åº¦ä¸åŒï¼ŒåŸºå‡†æ€§èƒ½ä¹Ÿä¸åŒã€‚

### 7.5 å¯¹æ¯” SFT å’Œ RL æ¨¡å‹

**åˆ›å»ºå¯¹æ¯”è„šæœ¬ `compare_results.py`**ï¼š

```python
import json
import sys

def load_results(file_path):
    with open(file_path, 'r') as f:
        return json.load(f)

def calculate_metrics(results, K=10):
    total = len(results)
    hr_at_k = 0
    ndcg_at_k = 0
    mrr = 0
    
    for item in results:
        target = item['output']
        predictions = item['predict'][:K]
        
        if target in predictions:
            hr_at_k += 1
            rank = predictions.index(target) + 1
            ndcg_at_k += 1.0 / (rank.bit_length())  # log2(rank+1)
            mrr += 1.0 / rank
    
    return {
        'HR@{}'.format(K): hr_at_k / total,
        'NDCG@{}'.format(K): ndcg_at_k / total,
        'MRR': mrr / total
    }

# åŠ è½½ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœ
sft_results = load_results('./output/sft_lora_qwen25_3b/eval_results.json')
rl_results = load_results('./output/rl_lora_qwen25_3b/eval_results.json')

# è®¡ç®—æŒ‡æ ‡
sft_metrics = calculate_metrics(sft_results, K=10)
rl_metrics = calculate_metrics(rl_results, K=10)

# è¾“å‡ºå¯¹æ¯”
print("=" * 60)
print("SFT vs RL æ¨¡å‹å¯¹æ¯”")
print("=" * 60)
print(f"{'æŒ‡æ ‡':<15} {'SFT æ¨¡å‹':<15} {'RL æ¨¡å‹':<15} {'æå‡':<15}")
print("-" * 60)

for key in ['HR@10', 'NDCG@10', 'MRR']:
    sft_val = sft_metrics[key]
    rl_val = rl_metrics[key]
    improvement = ((rl_val - sft_val) / sft_val) * 100
    print(f"{key:<15} {sft_val:<15.4f} {rl_val:<15.4f} {improvement:>+6.2f}%")

print("=" * 60)
```

è¿è¡Œå¯¹æ¯”ï¼š
```bash
python compare_results.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
============================================================
SFT vs RL æ¨¡å‹å¯¹æ¯”
============================================================
æŒ‡æ ‡             SFT æ¨¡å‹         RL æ¨¡å‹          æå‡
------------------------------------------------------------
HR@10           0.3200          0.3520          +10.00%
NDCG@10         0.1950          0.2156          +10.56%
MRR             0.1680          0.1834          +9.17%
============================================================
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- **RL æå‡ > 8%**ï¼šâœ… **RL è®­ç»ƒæ•ˆæœæ˜¾è‘—**
- **RL æå‡ 3-8%**ï¼šâš ï¸ **RL è®­ç»ƒæœ‰ä¸€å®šæ•ˆæœ**
- **RL æå‡ < 3%**ï¼šâŒ **RL è®­ç»ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°**
- **RL æå‡ < 0%**ï¼šâŒ **RL è®­ç»ƒå¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥é…ç½®**

### 7.6 æŸ¥çœ‹å…·ä½“é¢„æµ‹æ¡ˆä¾‹

**æŸ¥çœ‹é¢„æµ‹æ­£ç¡®çš„æ¡ˆä¾‹**ï¼š

```bash
python show_predictions.py --result_file ./output/sft_lora_qwen25_3b/eval_results.json --show_correct --limit 5
```

**æŸ¥çœ‹é¢„æµ‹é”™è¯¯çš„æ¡ˆä¾‹**ï¼š

```bash
python show_predictions.py --result_file ./output/sft_lora_qwen25_3b/eval_results.json --show_incorrect --limit 5
```

**ç¤ºä¾‹è¾“å‡º**ï¼š
```
æ¡ˆä¾‹ 1 (æ­£ç¡®é¢„æµ‹):
ç”¨æˆ·å†å²: <a_123><b_45><c_67>, <a_234><b_56><c_78>
çœŸå®ç›®æ ‡: <a_145><b_67><c_89>
Top-5 é¢„æµ‹:
  1. <a_145><b_67><c_89> âœ… (æ­£ç¡®ï¼æ’åç¬¬ 1)
  2. <a_150><b_70><c_90>
  3. <a_140><b_65><c_85>
  4. <a_155><b_72><c_92>
  5. <a_135><b_63><c_83>

æ¡ˆä¾‹ 2 (é”™è¯¯é¢„æµ‹):
ç”¨æˆ·å†å²: <a_456><b_78><c_90>, <a_567><b_89><c_01>
çœŸå®ç›®æ ‡: <a_678><b_90><c_12>
Top-5 é¢„æµ‹:
  1. <a_670><b_88><c_10>
  2. <a_680><b_92><c_14>
  3. <a_665><b_86><c_08>
  4. <a_685><b_94><c_16>
  5. <a_660><b_84><c_06>
ç›®æ ‡å•†å“æ’å: ç¬¬ 15 ä½ âŒ
```

### 7.7 è®­ç»ƒæ•ˆæœè¯Šæ–­

#### âœ… è®­ç»ƒæ•ˆæœè‰¯å¥½çš„æ ‡å¿—

1. **æŒ‡æ ‡è¾¾æ ‡**ï¼š
   - HR@10 â‰¥ 0.30
   - NDCG@10 â‰¥ 0.18
   - MRR â‰¥ 0.15

2. **RL æå‡æ˜æ˜¾**ï¼š
   - RL æ¯” SFT æå‡ > 5%

3. **è®­ç»ƒæŸå¤±æ”¶æ•›**ï¼š
   - SFT è®­ç»ƒæŸå¤±ç¨³å®šä¸‹é™
   - RL è®­ç»ƒ reward ç¨³å®šä¸Šå‡

4. **é¢„æµ‹å¤šæ ·æ€§**ï¼š
   - Top-K é¢„æµ‹ä¸é‡å¤
   - é¢„æµ‹ç»“æœç¬¦åˆè¯­ä¹‰

#### âŒ è®­ç»ƒæ•ˆæœä¸ä½³çš„æ ‡å¿—

1. **æŒ‡æ ‡è¿‡ä½**ï¼š
   - HR@10 < 0.20
   - NDCG@10 < 0.12
   - MRR < 0.10

2. **RL æ— æå‡æˆ–ä¸‹é™**ï¼š
   - RL æ¯” SFT æå‡ < 2%
   - RL æ¯” SFT ä¸‹é™

3. **è®­ç»ƒä¸ç¨³å®š**ï¼š
   - æŸå¤±éœ‡è¡
   - æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±

4. **é¢„æµ‹å¼‚å¸¸**ï¼š
   - é¢„æµ‹ç»“æœé‡å¤
   - é¢„æµ‹æ ¼å¼é”™è¯¯

#### ğŸ”§ æ”¹è¿›å»ºè®®

**å¦‚æœ HR@10 < 0.25**ï¼š
1. å¢åŠ è®­ç»ƒè½®æ•°ï¼š`--num_epochs 15`
2. å¢åŠ  LoRA rankï¼š`--lora_r 32`
3. è°ƒæ•´å­¦ä¹ ç‡ï¼š`--learning_rate 5e-4`
4. æ£€æŸ¥æ•°æ®è´¨é‡

**å¦‚æœ RL æ— æå‡**ï¼š
1. è°ƒæ•´ beta å‚æ•°ï¼š`--beta 0.02` æˆ– `0.08`
2. å¢åŠ ç”Ÿæˆæ ·æœ¬æ•°ï¼š`--num_generations 32`
3. è°ƒæ•´ RL å­¦ä¹ ç‡ï¼š`--learning_rate 5e-7`
4. å¢åŠ  RL è®­ç»ƒè½®æ•°ï¼š`--num_train_epochs 2`

**å¦‚æœè®­ç»ƒä¸ç¨³å®š**ï¼š
1. å‡å°å­¦ä¹ ç‡ï¼š`--learning_rate 1e-4`
2. å¢åŠ  warmupï¼šæ·»åŠ  warmup é…ç½®
3. ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼šæ£€æŸ¥ `max_grad_norm`
4. å‡å° batch size

### 7.8 å¿«é€Ÿè¯„ä¼°è„šæœ¬

åˆ›å»º `quick_eval.sh` ç”¨äºå¿«é€ŸæŸ¥çœ‹å…³é”®æŒ‡æ ‡ï¼š

```bash
#!/bin/bash

RESULT_FILE=$1

if [ -z "$RESULT_FILE" ]; then
    echo "ç”¨æ³•: bash quick_eval.sh <ç»“æœæ–‡ä»¶è·¯å¾„>"
    exit 1
fi

python -c "
import json
import math

with open('$RESULT_FILE', 'r') as f:
    results = json.load(f)

total = len(results)
hr1, hr5, hr10 = 0, 0, 0
ndcg10, mrr = 0, 0

for item in results:
    target = item['output']
    preds = item['predict']
    
    if target in preds[:1]: hr1 += 1
    if target in preds[:5]: hr5 += 1
    if target in preds[:10]: hr10 += 1
    
    if target in preds[:10]:
        rank = preds[:10].index(target) + 1
        ndcg10 += 1.0 / math.log2(rank + 1)
        mrr += 1.0 / rank

print('=' * 50)
print(f'æµ‹è¯•æ ·æœ¬æ•°: {total}')
print('=' * 50)
print(f'HR@1:  {hr1/total:.4f} ({hr1/total*100:.2f}%)')
print(f'HR@5:  {hr5/total:.4f} ({hr5/total*100:.2f}%)')
print(f'HR@10: {hr10/total:.4f} ({hr10/total*100:.2f}%)')
print(f'NDCG@10: {ndcg10/total:.4f}')
print(f'MRR: {mrr/total:.4f}')
print('=' * 50)

# åˆ¤æ–­æ•ˆæœ
if hr10/total >= 0.30:
    print('âœ… è®­ç»ƒæ•ˆæœï¼šè‰¯å¥½')
elif hr10/total >= 0.25:
    print('âš ï¸  è®­ç»ƒæ•ˆæœï¼šä¸€èˆ¬')
else:
    print('âŒ è®­ç»ƒæ•ˆæœï¼šè¾ƒå·®ï¼Œå»ºè®®è°ƒæ•´è¶…å‚æ•°')
print('=' * 50)
"
```

ä½¿ç”¨ï¼š
```bash
bash quick_eval.sh ./output/sft_lora_qwen25_3b/eval_results.json
bash quick_eval.sh ./output/rl_lora_qwen25_3b/eval_results.json
```

---

## ğŸ¯ å®Œæ•´å·¥ä½œæµç¨‹æ€»ç»“

```bash
# 1. ç¯å¢ƒé…ç½®
pip install -r requirements.txt

# 2. ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
# æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œæˆ–æ‰‹åŠ¨ä¸‹è½½åˆ° ./models/qwen3b

# 3. SFT è®­ç»ƒï¼ˆ2-4 å°æ—¶ï¼‰
bash sft.sh

# 4. è¯„ä¼° SFT æ¨¡å‹
bash evaluate_sft_lora.sh

# 5. RL è®­ç»ƒï¼ˆ1-2 å°æ—¶ï¼‰
bash rl_lora.sh

# 6. è¯„ä¼° RL æ¨¡å‹
bash evaluate_rl_lora.sh

# 7. å¯¹æ¯”ç»“æœ
cat ./output/sft_lora_qwen25_3b/eval_results.json
cat ./output/rl_lora_qwen25_3b/eval_results.json
```

---

### å¸¸è§é—®é¢˜

**1. æ¨¡å‹è·¯å¾„é”™è¯¯**

é”™è¯¯ï¼š`OSError: Can't load config for 'Qwen/Qwen2.5-3B-Instruct'`

è§£å†³ï¼š
- ç¡®è®¤æ¨¡å‹è·¯å¾„æ­£ç¡®
- å¦‚æœæ¨¡å‹åœ¨è‡ªå®šä¹‰ç›®å½•ï¼Œä½¿ç”¨å®Œæ•´è·¯å¾„ï¼š`./models/Qwen2.5-3B-Instruct`

**2. æ˜¾å­˜ä¸è¶³ï¼ˆOOMï¼‰**

è§£å†³ï¼š
- å‡å° `batch_size`ï¼šä» 1024 é™åˆ° 512
- å‡å° `micro_batch_size`ï¼šä» 32 é™åˆ° 16
- å‡å° `lora_r`ï¼šä» 16 é™åˆ° 8

**3. æ•°æ®æ–‡ä»¶æ‰¾ä¸åˆ°**

æ£€æŸ¥ï¼š
```bash
ls -lh ./data/Amazon/train/Industrial_and_Scientific*
ls -lh ./data/Amazon/index/Industrial_and_Scientific*
```

**4. è¯„ä¼°æ—¶è¯è¡¨å¤§å°ä¸åŒ¹é…**

é”™è¯¯ï¼š
```
RuntimeError: Error(s) in loading state_dict for Qwen2ForCausalLM:
    size mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([152225, 2048]) from checkpoint, the shape in current model is torch.Size([151936, 2048]).
```

è§£å†³ï¼š
- **ä¸è¦ä½¿ç”¨ `evaluate.py`**ï¼Œä½¿ç”¨ `evaluate_lora.py`
- è¿è¡Œï¼š`bash evaluate_sft_lora.sh` æˆ– `bash evaluate_rl_lora.sh`
- åŸå› ï¼šLoRA æ¨¡å‹åŒ…å«æ‰©å±•çš„è¯è¡¨ï¼Œéœ€è¦å…ˆåŠ è½½åŸºç¡€æ¨¡å‹ï¼Œè°ƒæ•´è¯è¡¨å¤§å°ï¼Œå†åŠ è½½ LoRA é€‚é…å™¨

**5. huggingface-hub ç‰ˆæœ¬å†²çª**

é”™è¯¯ï¼š
```
ImportError: huggingface-hub>=0.34.0,<1.0 is required
```

è§£å†³ï¼š
```bash
pip install "huggingface-hub>=0.34.0,<1.0"
```

**6. peft æ¨¡å—æœªå®‰è£…**

é”™è¯¯ï¼š
```
ModuleNotFoundError: No module named 'peft'
```

è§£å†³ï¼š
```bash
pip install peft
```

ç¥ä½ è®­ç»ƒé¡ºåˆ©ï¼ğŸš€

